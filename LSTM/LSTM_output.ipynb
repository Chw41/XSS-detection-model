{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:52.635407Z",
     "iopub.status.busy": "2024-12-02T12:43:52.634919Z",
     "iopub.status.idle": "2024-12-02T12:43:53.930805Z",
     "shell.execute_reply": "2024-12-02T12:43:53.930449Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:53.932616Z",
     "iopub.status.busy": "2024-12-02T12:43:53.932430Z",
     "iopub.status.idle": "2024-12-02T12:43:53.935873Z",
     "shell.execute_reply": "2024-12-02T12:43:53.935673Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_len: int = 100):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Convert input to string and clean it\n",
    "        text = str(text)\n",
    "        text = re.sub(r'([<>/=\"])', r' \\1 ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            # Ensure text is string\n",
    "            text = str(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq and word not in self.vocab:\n",
    "                self.vocab[word] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def encode_text(self, text: str) -> List[int]:\n",
    "        # Ensure text is string\n",
    "        text = str(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend(['<PAD>'] * (self.max_len - len(tokens)))\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:53.937315Z",
     "iopub.status.busy": "2024-12-02T12:43:53.937177Z",
     "iopub.status.idle": "2024-12-02T12:43:53.941294Z",
     "shell.execute_reply": "2024-12-02T12:43:53.941053Z"
    }
   },
   "outputs": [],
   "source": [
    "class XSSDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], preprocessor: TextPreprocessor):\n",
    "        # Convert all texts to strings\n",
    "        self.texts = [str(text) for text in texts]\n",
    "        self.preprocessor = preprocessor\n",
    "        self.encodings = [self.preprocessor.encode_text(text) for text in self.texts]\n",
    "        self.labels = [int(label) for label in labels]  # Convert labels to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "\n",
    "def load_and_clean_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load and clean the dataset, ensuring proper data types.\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert texts to strings and clean them\n",
    "        texts = [str(text).strip() for text in data['Sentence']]\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        labels = [int(label) for label in data['Label']]\n",
    "        \n",
    "        # Basic validation\n",
    "        assert len(texts) == len(labels), \"Number of texts and labels must match\"\n",
    "        assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n",
    "        assert all(isinstance(label, int) and label in [0, 1] for label in labels), \"Labels must be binary (0 or 1)\"\n",
    "        \n",
    "        print(f\"Loaded {len(texts)} samples successfully\")\n",
    "        \n",
    "        # Print some basic statistics\n",
    "        print(f\"Number of positive samples: {sum(labels)}\")\n",
    "        print(f\"Number of negative samples: {len(labels) - sum(labels)}\")\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:53.942793Z",
     "iopub.status.busy": "2024-12-02T12:43:53.942604Z",
     "iopub.status.idle": "2024-12-02T12:43:53.945214Z",
     "shell.execute_reply": "2024-12-02T12:43:53.944994Z"
    }
   },
   "outputs": [],
   "source": [
    "class XSSDetectorLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 50, \n",
    "                 hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:53.946750Z",
     "iopub.status.busy": "2024-12-02T12:43:53.946608Z",
     "iopub.status.idle": "2024-12-02T12:43:53.954706Z",
     "shell.execute_reply": "2024-12-02T12:43:53.954451Z"
    }
   },
   "outputs": [],
   "source": [
    "class XSSDetector:\n",
    "    def __init__(self, max_len: int = 100, device: str = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else torch.device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.preprocessor = TextPreprocessor(max_len)\n",
    "        self.model = None\n",
    "        self.results = {}\n",
    "    \n",
    "    def train(self, texts: List[str], labels: List[int], \n",
    "              epochs: int = 20, batch_size: int = 16,  \n",
    "              learning_rates: List[float] = [0.001, 0.002, 0.01, 0.02, 0.05]):\n",
    "        \n",
    "        try:\n",
    "            texts = [str(text) for text in texts]\n",
    "            labels = torch.tensor(labels, dtype=torch.float)\n",
    "            \n",
    "            self.preprocessor.build_vocab(texts)\n",
    "            dataset = XSSDataset(texts, labels.numpy(), self.preprocessor)\n",
    "            \n",
    "            # Split dataset\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.2 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "            \n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataset splits:\")\n",
    "            print(f\"Training: {train_size} samples\")\n",
    "            print(f\"Validation: {val_size} samples\")\n",
    "            print(f\"Test: {test_size} samples\")\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            # Train with different learning rates\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\nTraining with learning rate: {lr}\")\n",
    "                print(f\"Current learning rate: {lr}\")\n",
    "                \n",
    "                self.model = XSSDetectorLSTM(\n",
    "                    vocab_size=self.preprocessor.vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    dropout=0.3  # Increased dropout for regularization\n",
    "                ).to(self.device)\n",
    "                \n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                criterion = nn.BCELoss()\n",
    "                \n",
    "                # Learning rate scheduler for better convergence\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "                )\n",
    "                \n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training\n",
    "                    self.model.train()\n",
    "                    total_loss = 0\n",
    "                    train_correct = 0\n",
    "                    train_total = 0\n",
    "                    \n",
    "                    for batch_sequences, batch_labels in train_loader:\n",
    "                        batch_sequences = batch_sequences.to(self.device)\n",
    "                        batch_labels = batch_labels.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = self.model(batch_sequences).squeeze()\n",
    "                        loss = criterion(outputs, batch_labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        predictions = (outputs >= 0.5).float()\n",
    "                        train_correct += (predictions == batch_labels).sum().item()\n",
    "                        train_total += len(batch_labels)\n",
    "                    \n",
    "                    avg_train_loss = total_loss / len(train_loader)\n",
    "                    train_losses.append(avg_train_loss)\n",
    "                    \n",
    "                    # Validation\n",
    "                    self.model.eval()\n",
    "                    val_loss = 0\n",
    "                    val_correct = 0\n",
    "                    val_total = 0\n",
    "                    val_predictions = []\n",
    "                    val_true_labels = []\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_sequences, batch_labels in val_loader:\n",
    "                            batch_sequences = batch_sequences.to(self.device)\n",
    "                            batch_labels = batch_labels.to(self.device)\n",
    "                            \n",
    "                            outputs = self.model(batch_sequences).squeeze()\n",
    "                            batch_val_loss = criterion(outputs, batch_labels).item()\n",
    "                            val_loss += batch_val_loss\n",
    "                            \n",
    "                            predictions = (outputs >= 0.5).float()\n",
    "                            val_correct += (predictions == batch_labels).sum().item()\n",
    "                            val_total += len(batch_labels)\n",
    "                            \n",
    "                            val_predictions.extend(predictions.cpu().numpy())\n",
    "                            val_true_labels.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    \n",
    "                    # Learning rate scheduler step with current validation loss\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch+1}, Current LR: {current_lr:.6f}\")\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        patience_counter = 0\n",
    "                        # Save the best model\n",
    "                        torch.save({\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'preprocessor': self.preprocessor\n",
    "                        }, f'best_model_lr_{lr}.pth')\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    if patience_counter > 5:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                    \n",
    "                    if (epoch + 1) % 5 == 0:\n",
    "                        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "                        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "                        print(f'Training Accuracy: {100*train_correct/train_total:.2f}%')\n",
    "                        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "                        print(f'Validation Accuracy: {100*val_correct/val_total:.2f}%')\n",
    "                \n",
    "                # Final evaluation\n",
    "                val_f1 = f1_score(val_true_labels, val_predictions)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[lr] = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'final_train_loss': avg_train_loss,\n",
    "                    'final_val_loss': avg_val_loss,\n",
    "                    'train_accuracy': 100 * train_correct / train_total,\n",
    "                    'val_accuracy': 100 * val_correct / val_total,\n",
    "                    'f1_score': val_f1\n",
    "                }\n",
    "                \n",
    "                # Plot normalized training curves\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(range(1, len(train_losses) + 1), \n",
    "                         [loss/train_losses[0] for loss in train_losses], \n",
    "                         label='Normalized Training Loss')\n",
    "                plt.plot(range(1, len(val_losses) + 1), \n",
    "                         [loss/val_losses[0] for loss in val_losses], \n",
    "                         label='Normalized Validation Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss (Normalized to First Epoch)')\n",
    "                plt.title(f'Normalized Training and Validation Loss (Learning Rate: {lr})')\n",
    "                plt.ylim(0, 8)  # Allow variation between 0 and 8 times the initial loss\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'normalized_loss_plot_lr_{lr}.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # Print final results\n",
    "            print(\"\\nFinal Results for All Learning Rates:\")\n",
    "            print(\"\\nLR      Train Loss  Val Loss    Train Acc   Val Acc    F1 Score\")\n",
    "            print(\"-\" * 65)\n",
    "            for lr in learning_rates:\n",
    "                r = self.results[lr]\n",
    "                print(f\"{lr:.3f}  {r['final_train_loss']:.4f}     {r['final_val_loss']:.4f}     \"\n",
    "                      f\"{r['train_accuracy']:.2f}%     {r['val_accuracy']:.2f}%     {r['f1_score']:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Training error: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:53.956155Z",
     "iopub.status.busy": "2024-12-02T12:43:53.956003Z",
     "iopub.status.idle": "2024-12-02T12:43:53.958424Z",
     "shell.execute_reply": "2024-12-02T12:43:53.958196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-execution environment check\n",
    "def check_environment():\n",
    "    print(\"\\n--- Environment Check ---\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    try:\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    except:\n",
    "        print(\"No CUDA device currently selected\")\n",
    "\n",
    "def demo_detector(dataset_path='Training Dataset/final_dataset.csv'):\n",
    "    print(\"\\n--- XSS Detection Model Demonstration ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load and clean data\n",
    "        texts, labels = load_and_clean_data(dataset_path)\n",
    "        \n",
    "        # Initialize and train detector\n",
    "        detector = XSSDetector(max_len=100)\n",
    "        detector.train(\n",
    "            texts=texts,\n",
    "            labels=labels,\n",
    "            epochs=20,\n",
    "            batch_size=16,\n",
    "            learning_rates=[0.001, 0.002, 0.01, 0.02, 0.05]\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Demonstration failed: {e}\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. Ensure correct dataset path\")\n",
    "        print(\"2. Check dataset format\")\n",
    "        print(\"3. Verify required libraries are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T12:43:53.959874Z",
     "iopub.status.busy": "2024-12-02T12:43:53.959699Z",
     "iopub.status.idle": "2024-12-02T12:56:16.026892Z",
     "shell.execute_reply": "2024-12-02T12:56:16.026448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Environment Check ---\n",
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "\n",
      "--- XSS Detection Model Demonstration ---\n",
      "Loaded 88310 samples successfully\n",
      "Number of positive samples: 50590\n",
      "Number of negative samples: 37720\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits:\n",
      "Training: 61816 samples\n",
      "Validation: 17662 samples\n",
      "Test: 8832 samples\n",
      "\n",
      "Training with learning rate: 0.001\n",
      "Current learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Current LR: 0.001000\n",
      "Epoch 5/20:\n",
      "Training Loss: 0.0284\n",
      "Training Accuracy: 99.05%\n",
      "Validation Loss: 0.0400\n",
      "Validation Accuracy: 98.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Current LR: 0.000500\n",
      "Epoch 10/20:\n",
      "Training Loss: 0.0245\n",
      "Training Accuracy: 99.18%\n",
      "Validation Loss: 0.0383\n",
      "Validation Accuracy: 98.83%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Current LR: 0.000500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Current LR: 0.000500\n",
      "Early stopping triggered\n",
      "\n",
      "Training with learning rate: 0.002\n",
      "Current learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Current LR: 0.002000\n",
      "Epoch 5/20:\n",
      "Training Loss: 0.0290\n",
      "Training Accuracy: 99.04%\n",
      "Validation Loss: 0.0430\n",
      "Validation Accuracy: 98.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Current LR: 0.002000\n",
      "Epoch 10/20:\n",
      "Training Loss: 0.0275\n",
      "Training Accuracy: 99.09%\n",
      "Validation Loss: 0.0345\n",
      "Validation Accuracy: 98.98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Current LR: 0.002000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Current LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Current LR: 0.001000\n",
      "Early stopping triggered\n",
      "\n",
      "Training with learning rate: 0.01\n",
      "Current learning rate: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Current LR: 0.010000\n",
      "Epoch 5/20:\n",
      "Training Loss: 0.0498\n",
      "Training Accuracy: 98.46%\n",
      "Validation Loss: 0.0521\n",
      "Validation Accuracy: 98.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Current LR: 0.010000\n",
      "Epoch 10/20:\n",
      "Training Loss: 0.0459\n",
      "Training Accuracy: 98.57%\n",
      "Validation Loss: 0.0496\n",
      "Validation Accuracy: 98.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Current LR: 0.005000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Current LR: 0.005000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Current LR: 0.005000\n",
      "Epoch 15/20:\n",
      "Training Loss: 0.0332\n",
      "Training Accuracy: 98.91%\n",
      "Validation Loss: 0.0401\n",
      "Validation Accuracy: 98.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Current LR: 0.005000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Current LR: 0.005000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Current LR: 0.005000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Current LR: 0.005000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Current LR: 0.005000\n",
      "Epoch 20/20:\n",
      "Training Loss: 0.0305\n",
      "Training Accuracy: 99.00%\n",
      "Validation Loss: 0.0386\n",
      "Validation Accuracy: 98.73%\n",
      "\n",
      "Training with learning rate: 0.02\n",
      "Current learning rate: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Current LR: 0.020000\n",
      "Epoch 5/20:\n",
      "Training Loss: 0.0842\n",
      "Training Accuracy: 97.39%\n",
      "Validation Loss: 0.0641\n",
      "Validation Accuracy: 97.98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Current LR: 0.020000\n",
      "Epoch 10/20:\n",
      "Training Loss: 0.0791\n",
      "Training Accuracy: 97.61%\n",
      "Validation Loss: 0.0636\n",
      "Validation Accuracy: 98.07%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Current LR: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Current LR: 0.010000\n",
      "Epoch 15/20:\n",
      "Training Loss: 0.0598\n",
      "Training Accuracy: 98.14%\n",
      "Validation Loss: 0.0474\n",
      "Validation Accuracy: 98.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Current LR: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Current LR: 0.010000\n",
      "Epoch 20/20:\n",
      "Training Loss: 0.0557\n",
      "Training Accuracy: 98.40%\n",
      "Validation Loss: 0.0474\n",
      "Validation Accuracy: 98.58%\n",
      "\n",
      "Training with learning rate: 0.05\n",
      "Current learning rate: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Current LR: 0.050000\n",
      "Epoch 5/20:\n",
      "Training Loss: 0.1770\n",
      "Training Accuracy: 94.66%\n",
      "Validation Loss: 0.1023\n",
      "Validation Accuracy: 96.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Current LR: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Current LR: 0.025000\n",
      "Epoch 10/20:\n",
      "Training Loss: 0.1176\n",
      "Training Accuracy: 96.45%\n",
      "Validation Loss: 0.0919\n",
      "Validation Accuracy: 97.51%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Current LR: 0.025000\n",
      "Epoch 15/20:\n",
      "Training Loss: 0.1073\n",
      "Training Accuracy: 96.86%\n",
      "Validation Loss: 0.0842\n",
      "Validation Accuracy: 97.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Current LR: 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Current LR: 0.012500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Current LR: 0.012500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Current LR: 0.012500\n",
      "Epoch 20/20:\n",
      "Training Loss: 0.0865\n",
      "Training Accuracy: 97.41%\n",
      "Validation Loss: 0.0748\n",
      "Validation Accuracy: 97.71%\n",
      "\n",
      "Final Results for All Learning Rates:\n",
      "\n",
      "LR      Train Loss  Val Loss    Train Acc   Val Acc    F1 Score\n",
      "-----------------------------------------------------------------\n",
      "0.001  0.0210     0.0389     99.28%     98.91%     0.9904\n",
      "0.002  0.0230     0.0349     99.21%     98.92%     0.9904\n",
      "0.010  0.0305     0.0386     99.00%     98.73%     0.9888\n",
      "0.020  0.0557     0.0474     98.40%     98.58%     0.9875\n",
      "0.050  0.0865     0.0748     97.41%     97.71%     0.9796\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n",
    "    demo_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
