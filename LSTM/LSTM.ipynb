{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_len: int = 100):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Convert input to string and clean it\n",
    "        text = str(text)\n",
    "        text = re.sub(r'([<>/=\"])', r' \\1 ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            # Ensure text is string\n",
    "            text = str(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq and word not in self.vocab:\n",
    "                self.vocab[word] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def encode_text(self, text: str) -> List[int]:\n",
    "        # Ensure text is string\n",
    "        text = str(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend(['<PAD>'] * (self.max_len - len(tokens)))\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], preprocessor: TextPreprocessor):\n",
    "        # Convert all texts to strings\n",
    "        self.texts = [str(text) for text in texts]\n",
    "        self.preprocessor = preprocessor\n",
    "        self.encodings = [self.preprocessor.encode_text(text) for text in self.texts]\n",
    "        self.labels = [int(label) for label in labels]  # Convert labels to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "\n",
    "def load_and_clean_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load and clean the dataset, ensuring proper data types.\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert texts to strings and clean them\n",
    "        texts = [str(text).strip() for text in data['Sentence']]\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        labels = [int(label) for label in data['Label']]\n",
    "        \n",
    "        # Basic validation\n",
    "        assert len(texts) == len(labels), \"Number of texts and labels must match\"\n",
    "        assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n",
    "        assert all(isinstance(label, int) and label in [0, 1] for label in labels), \"Labels must be binary (0 or 1)\"\n",
    "        \n",
    "        print(f\"Loaded {len(texts)} samples successfully\")\n",
    "        \n",
    "        # Print some basic statistics\n",
    "        print(f\"Number of positive samples: {sum(labels)}\")\n",
    "        print(f\"Number of negative samples: {len(labels) - sum(labels)}\")\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetectorLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 50, \n",
    "                 hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetector:\n",
    "    def __init__(self, max_len: int = 100, device: str = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else torch.device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.preprocessor = TextPreprocessor(max_len)\n",
    "        self.model = None\n",
    "        self.results = {}\n",
    "\n",
    "    def plot_loss_curves(self):\n",
    "        \"\"\"Plot loss curve visualizations comparing different learning rates.\"\"\"\n",
    "        # Individual plots for each learning rate\n",
    "        for lr in self.results.keys():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            train_losses = self.results[lr]['train_losses']\n",
    "            val_losses = self.results[lr]['val_losses']\n",
    "            epochs = range(1, len(train_losses) + 1)\n",
    "            \n",
    "            plt.plot(epochs, train_losses, label='Training Loss', marker='o', markersize=4)\n",
    "            plt.plot(epochs, val_losses, label='Validation Loss', marker='o', markersize=4)\n",
    "            \n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Training and Validation Loss (Learning Rate: {lr})')\n",
    "            plt.xticks(np.arange(0, len(train_losses) + 1, 5))\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'LSTM_loss_plot_lr_{lr}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Combined training losses plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lr in self.results.keys():\n",
    "            train_losses = self.results[lr]['train_losses']\n",
    "            epochs = range(1, len(train_losses) + 1)\n",
    "            plt.plot(epochs, train_losses, label=f'LR = {lr}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Training Loss Comparison Across Learning Rates')\n",
    "        plt.xticks(np.arange(0, max(len(self.results[lr]['train_losses']) for lr in self.results.keys()) + 1, 5))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('LSTM_combined_training_losses.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Combined validation losses plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lr in self.results.keys():\n",
    "            val_losses = self.results[lr]['val_losses']\n",
    "            epochs = range(1, len(val_losses) + 1)\n",
    "            plt.plot(epochs, val_losses, label=f'LR = {lr}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Validation Loss Comparison Across Learning Rates')\n",
    "        plt.xticks(np.arange(0, max(len(self.results[lr]['val_losses']) for lr in self.results.keys()) + 1, 5))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('LSTM_combined_validation_losses.png')\n",
    "        plt.close()\n",
    "\n",
    "    def train(self, texts: List[str], labels: List[int], \n",
    "              epochs: int = 50, batch_size: int = 16,  \n",
    "              learning_rates: List[float] = [0.001, 0.002, 0.01, 0.02, 0.05]):\n",
    "        try:\n",
    "            texts = [str(text) for text in texts]\n",
    "            labels = torch.tensor(labels, dtype=torch.float)\n",
    "            \n",
    "            self.preprocessor.build_vocab(texts)\n",
    "            dataset = XSSDataset(texts, labels.numpy(), self.preprocessor)\n",
    "            \n",
    "            # Split dataset\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.2 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "            \n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataset splits:\")\n",
    "            print(f\"Training: {train_size} samples\")\n",
    "            print(f\"Validation: {val_size} samples\")\n",
    "            print(f\"Test: {test_size} samples\")\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            # Import metrics calculation functions\n",
    "            from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "            \n",
    "            # Train with different learning rates\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\n--- Learning Rate: {lr} ---\")\n",
    "                \n",
    "                self.model = XSSDetectorLSTM(\n",
    "                    vocab_size=self.preprocessor.vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    dropout=0.3\n",
    "                ).to(self.device)\n",
    "                \n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                criterion = nn.BCELoss()\n",
    "                \n",
    "                # Learning rate scheduler for better convergence\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "                )\n",
    "                \n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training\n",
    "                    self.model.train()\n",
    "                    total_loss = 0\n",
    "                    train_correct = 0\n",
    "                    train_total = 0\n",
    "                    \n",
    "                    for batch_sequences, batch_labels in train_loader:\n",
    "                        batch_sequences = batch_sequences.to(self.device)\n",
    "                        batch_labels = batch_labels.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = self.model(batch_sequences).squeeze()\n",
    "                        loss = criterion(outputs, batch_labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        predictions = (outputs >= 0.5).float()\n",
    "                        train_correct += (predictions == batch_labels).sum().item()\n",
    "                        train_total += len(batch_labels)\n",
    "                    \n",
    "                    avg_train_loss = total_loss / len(train_loader)\n",
    "                    train_losses.append(avg_train_loss)\n",
    "                    \n",
    "                    # Validation\n",
    "                    self.model.eval()\n",
    "                    val_loss = 0\n",
    "                    val_predictions = []\n",
    "                    val_true_labels = []\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_sequences, batch_labels in val_loader:\n",
    "                            batch_sequences = batch_sequences.to(self.device)\n",
    "                            batch_labels = batch_labels.to(self.device)\n",
    "                            \n",
    "                            outputs = self.model(batch_sequences).squeeze()\n",
    "                            batch_val_loss = criterion(outputs, batch_labels).item()\n",
    "                            val_loss += batch_val_loss\n",
    "                            \n",
    "                            predictions = (outputs >= 0.5).float()\n",
    "                            val_predictions.extend(predictions.cpu().numpy())\n",
    "                            val_true_labels.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    \n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "                    \n",
    "                    # Learning rate scheduler step\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        patience_counter = 0\n",
    "                        # Save best model\n",
    "                        torch.save({\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'preprocessor': self.preprocessor\n",
    "                        }, f'LSTM_model_lr_{lr}.pth')\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    if patience_counter > 50:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                # Calculate final metrics\n",
    "                val_predictions = np.array(val_predictions)\n",
    "                val_true_labels = np.array(val_true_labels)\n",
    "                \n",
    "                f1 = f1_score(val_true_labels, val_predictions)\n",
    "                accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "                precision = precision_score(val_true_labels, val_predictions)\n",
    "                recall = recall_score(val_true_labels, val_predictions)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[lr] = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'f1_score': f1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall\n",
    "                }\n",
    "            \n",
    "            # Print comprehensive results\n",
    "            print(\"\\n--- Comprehensive Results ---\")\n",
    "            for lr in learning_rates:\n",
    "                r = self.results[lr]\n",
    "                print(f\"\\nLearning Rate: {lr}\")\n",
    "                print(f\"F1 Score: {r['f1_score']:.16f}\")\n",
    "                print(f\"Accuracy: {r['accuracy']:.16f}\")\n",
    "                print(f\"Precision: {r['precision']:.16f}\")\n",
    "                print(f\"Recall: {r['recall']:.16f}\")\n",
    "            \n",
    "            # Plot all loss curves\n",
    "            self.plot_loss_curves()\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Training error: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-execution environment check\n",
    "def check_environment():\n",
    "    print(\"\\n--- Environment Check ---\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    try:\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    except:\n",
    "        print(\"No CUDA device currently selected\")\n",
    "\n",
    "def demo_detector(dataset_path='../Training Dataset/final_dataset.csv'):\n",
    "    print(\"\\n--- XSS Detection Model Demonstration ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load and clean data\n",
    "        texts, labels = load_and_clean_data(dataset_path)\n",
    "        \n",
    "        # Initialize and train detector\n",
    "        detector = XSSDetector(max_len=100)\n",
    "        detector.train(\n",
    "            texts=texts,\n",
    "            labels=labels,\n",
    "            epochs=50,\n",
    "            batch_size=16,\n",
    "            learning_rates=[0.001, 0.002, 0.01, 0.02, 0.05]\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Demonstration failed: {e}\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. Ensure correct dataset path\")\n",
    "        print(\"2. Check dataset format\")\n",
    "        print(\"3. Verify required libraries are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Environment Check ---\n",
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "\n",
      "--- XSS Detection Model Demonstration ---\n",
      "Loaded 88310 samples successfully\n",
      "Number of positive samples: 50590\n",
      "Number of negative samples: 37720\n",
      "Using device: cuda\n",
      "\n",
      "Dataset splits:\n",
      "Training: 61816 samples\n",
      "Validation: 17662 samples\n",
      "Test: 8832 samples\n",
      "\n",
      "--- Learning Rate: 0.001 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0838, Val Loss = 0.0535\n",
      "Epoch 2/50: Train Loss = 0.0438, Val Loss = 0.0430\n",
      "Epoch 3/50: Train Loss = 0.0339, Val Loss = 0.0404\n",
      "Epoch 4/50: Train Loss = 0.0295, Val Loss = 0.0417\n",
      "Epoch 5/50: Train Loss = 0.0272, Val Loss = 0.0414\n",
      "Epoch 6/50: Train Loss = 0.0272, Val Loss = 0.0426\n",
      "Epoch 7/50: Train Loss = 0.0258, Val Loss = 0.0412\n",
      "Epoch 8/50: Train Loss = 0.0221, Val Loss = 0.0451\n",
      "Epoch 9/50: Train Loss = 0.0232, Val Loss = 0.0429\n",
      "Epoch 10/50: Train Loss = 0.0225, Val Loss = 0.0427\n",
      "Epoch 11/50: Train Loss = 0.0221, Val Loss = 0.0439\n",
      "Epoch 12/50: Train Loss = 0.0208, Val Loss = 0.0434\n",
      "Epoch 13/50: Train Loss = 0.0193, Val Loss = 0.0459\n",
      "Epoch 14/50: Train Loss = 0.0201, Val Loss = 0.0449\n",
      "Epoch 15/50: Train Loss = 0.0202, Val Loss = 0.0409\n",
      "Epoch 16/50: Train Loss = 0.0187, Val Loss = 0.0445\n",
      "Epoch 17/50: Train Loss = 0.0182, Val Loss = 0.0472\n",
      "Epoch 18/50: Train Loss = 0.0183, Val Loss = 0.0468\n",
      "Epoch 19/50: Train Loss = 0.0187, Val Loss = 0.0461\n",
      "Epoch 20/50: Train Loss = 0.0172, Val Loss = 0.0503\n",
      "Epoch 21/50: Train Loss = 0.0175, Val Loss = 0.0522\n",
      "Epoch 22/50: Train Loss = 0.0178, Val Loss = 0.0518\n",
      "Epoch 23/50: Train Loss = 0.0177, Val Loss = 0.0526\n",
      "Epoch 24/50: Train Loss = 0.0170, Val Loss = 0.0544\n",
      "Epoch 25/50: Train Loss = 0.0167, Val Loss = 0.0535\n",
      "Epoch 26/50: Train Loss = 0.0170, Val Loss = 0.0530\n",
      "Epoch 27/50: Train Loss = 0.0172, Val Loss = 0.0554\n",
      "Epoch 28/50: Train Loss = 0.0167, Val Loss = 0.0551\n",
      "Epoch 29/50: Train Loss = 0.0166, Val Loss = 0.0542\n",
      "Epoch 30/50: Train Loss = 0.0169, Val Loss = 0.0551\n",
      "Epoch 31/50: Train Loss = 0.0166, Val Loss = 0.0561\n",
      "Epoch 32/50: Train Loss = 0.0167, Val Loss = 0.0564\n",
      "Epoch 33/50: Train Loss = 0.0168, Val Loss = 0.0562\n",
      "Epoch 34/50: Train Loss = 0.0164, Val Loss = 0.0581\n",
      "Epoch 35/50: Train Loss = 0.0168, Val Loss = 0.0572\n",
      "Epoch 36/50: Train Loss = 0.0165, Val Loss = 0.0576\n",
      "Epoch 37/50: Train Loss = 0.0165, Val Loss = 0.0573\n",
      "Epoch 38/50: Train Loss = 0.0164, Val Loss = 0.0579\n",
      "Epoch 39/50: Train Loss = 0.0165, Val Loss = 0.0572\n",
      "Epoch 40/50: Train Loss = 0.0165, Val Loss = 0.0576\n",
      "Epoch 41/50: Train Loss = 0.0164, Val Loss = 0.0577\n",
      "Epoch 42/50: Train Loss = 0.0165, Val Loss = 0.0577\n",
      "Epoch 43/50: Train Loss = 0.0164, Val Loss = 0.0577\n",
      "Epoch 44/50: Train Loss = 0.0164, Val Loss = 0.0579\n",
      "Epoch 45/50: Train Loss = 0.0163, Val Loss = 0.0580\n",
      "Epoch 46/50: Train Loss = 0.0164, Val Loss = 0.0581\n",
      "Epoch 47/50: Train Loss = 0.0165, Val Loss = 0.0581\n",
      "Epoch 48/50: Train Loss = 0.0165, Val Loss = 0.0581\n",
      "Epoch 49/50: Train Loss = 0.0162, Val Loss = 0.0582\n",
      "Epoch 50/50: Train Loss = 0.0163, Val Loss = 0.0583\n",
      "\n",
      "--- Learning Rate: 0.002 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0725, Val Loss = 0.0475\n",
      "Epoch 2/50: Train Loss = 0.0384, Val Loss = 0.0487\n",
      "Epoch 3/50: Train Loss = 0.0315, Val Loss = 0.0431\n",
      "Epoch 4/50: Train Loss = 0.0303, Val Loss = 0.0407\n",
      "Epoch 5/50: Train Loss = 0.0286, Val Loss = 0.0419\n",
      "Epoch 6/50: Train Loss = 0.0280, Val Loss = 0.0414\n",
      "Epoch 7/50: Train Loss = 0.0271, Val Loss = 0.0448\n",
      "Epoch 8/50: Train Loss = 0.0269, Val Loss = 0.0414\n",
      "Epoch 9/50: Train Loss = 0.0228, Val Loss = 0.0406\n",
      "Epoch 10/50: Train Loss = 0.0225, Val Loss = 0.0447\n",
      "Epoch 11/50: Train Loss = 0.0234, Val Loss = 0.0415\n",
      "Epoch 12/50: Train Loss = 0.0226, Val Loss = 0.0415\n",
      "Epoch 13/50: Train Loss = 0.0227, Val Loss = 0.0407\n",
      "Epoch 14/50: Train Loss = 0.0203, Val Loss = 0.0429\n",
      "Epoch 15/50: Train Loss = 0.0199, Val Loss = 0.0440\n",
      "Epoch 16/50: Train Loss = 0.0206, Val Loss = 0.0409\n",
      "Epoch 17/50: Train Loss = 0.0203, Val Loss = 0.0397\n",
      "Epoch 18/50: Train Loss = 0.0200, Val Loss = 0.0432\n",
      "Epoch 19/50: Train Loss = 0.0200, Val Loss = 0.0412\n",
      "Epoch 20/50: Train Loss = 0.0204, Val Loss = 0.0419\n",
      "Epoch 21/50: Train Loss = 0.0205, Val Loss = 0.0427\n",
      "Epoch 22/50: Train Loss = 0.0181, Val Loss = 0.0444\n",
      "Epoch 23/50: Train Loss = 0.0180, Val Loss = 0.0461\n",
      "Epoch 24/50: Train Loss = 0.0185, Val Loss = 0.0465\n",
      "Epoch 25/50: Train Loss = 0.0186, Val Loss = 0.0454\n",
      "Epoch 26/50: Train Loss = 0.0170, Val Loss = 0.0485\n",
      "Epoch 27/50: Train Loss = 0.0173, Val Loss = 0.0481\n",
      "Epoch 28/50: Train Loss = 0.0172, Val Loss = 0.0506\n",
      "Epoch 29/50: Train Loss = 0.0172, Val Loss = 0.0508\n",
      "Epoch 30/50: Train Loss = 0.0167, Val Loss = 0.0495\n",
      "Epoch 31/50: Train Loss = 0.0165, Val Loss = 0.0506\n",
      "Epoch 32/50: Train Loss = 0.0164, Val Loss = 0.0513\n",
      "Epoch 33/50: Train Loss = 0.0166, Val Loss = 0.0520\n",
      "Epoch 34/50: Train Loss = 0.0161, Val Loss = 0.0512\n",
      "Epoch 35/50: Train Loss = 0.0162, Val Loss = 0.0538\n",
      "Epoch 36/50: Train Loss = 0.0160, Val Loss = 0.0539\n",
      "Epoch 37/50: Train Loss = 0.0160, Val Loss = 0.0555\n",
      "Epoch 38/50: Train Loss = 0.0159, Val Loss = 0.0558\n",
      "Epoch 39/50: Train Loss = 0.0159, Val Loss = 0.0550\n",
      "Epoch 40/50: Train Loss = 0.0160, Val Loss = 0.0558\n",
      "Epoch 41/50: Train Loss = 0.0159, Val Loss = 0.0557\n",
      "Epoch 42/50: Train Loss = 0.0158, Val Loss = 0.0557\n",
      "Epoch 43/50: Train Loss = 0.0160, Val Loss = 0.0561\n",
      "Epoch 44/50: Train Loss = 0.0158, Val Loss = 0.0557\n",
      "Epoch 45/50: Train Loss = 0.0158, Val Loss = 0.0560\n",
      "Epoch 46/50: Train Loss = 0.0157, Val Loss = 0.0561\n",
      "Epoch 47/50: Train Loss = 0.0157, Val Loss = 0.0567\n",
      "Epoch 48/50: Train Loss = 0.0158, Val Loss = 0.0565\n",
      "Epoch 49/50: Train Loss = 0.0155, Val Loss = 0.0564\n",
      "Epoch 50/50: Train Loss = 0.0156, Val Loss = 0.0564\n",
      "\n",
      "--- Learning Rate: 0.01 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0709, Val Loss = 0.0570\n",
      "Epoch 2/50: Train Loss = 0.0493, Val Loss = 0.0594\n",
      "Epoch 3/50: Train Loss = 0.0501, Val Loss = 0.0537\n",
      "Epoch 4/50: Train Loss = 0.0540, Val Loss = 0.0519\n",
      "Epoch 5/50: Train Loss = 0.0521, Val Loss = 0.0537\n",
      "Epoch 6/50: Train Loss = 0.0494, Val Loss = 0.0509\n",
      "Epoch 7/50: Train Loss = 0.0474, Val Loss = 0.0541\n",
      "Epoch 8/50: Train Loss = 0.0485, Val Loss = 0.0535\n",
      "Epoch 9/50: Train Loss = 0.0486, Val Loss = 0.0560\n",
      "Epoch 10/50: Train Loss = 0.0459, Val Loss = 0.0514\n",
      "Epoch 11/50: Train Loss = 0.0372, Val Loss = 0.0544\n",
      "Epoch 12/50: Train Loss = 0.0353, Val Loss = 0.0492\n",
      "Epoch 13/50: Train Loss = 0.0372, Val Loss = 0.0484\n",
      "Epoch 14/50: Train Loss = 0.0346, Val Loss = 0.0528\n",
      "Epoch 15/50: Train Loss = 0.0337, Val Loss = 0.0422\n",
      "Epoch 16/50: Train Loss = 0.0326, Val Loss = 0.0467\n",
      "Epoch 17/50: Train Loss = 0.0322, Val Loss = 0.0482\n",
      "Epoch 18/50: Train Loss = 0.0313, Val Loss = 0.0429\n",
      "Epoch 19/50: Train Loss = 0.0316, Val Loss = 0.0421\n",
      "Epoch 20/50: Train Loss = 0.0290, Val Loss = 0.0442\n",
      "Epoch 21/50: Train Loss = 0.0309, Val Loss = 0.0428\n",
      "Epoch 22/50: Train Loss = 0.0296, Val Loss = 0.0470\n",
      "Epoch 23/50: Train Loss = 0.0293, Val Loss = 0.0461\n",
      "Epoch 24/50: Train Loss = 0.0247, Val Loss = 0.0437\n",
      "Epoch 25/50: Train Loss = 0.0242, Val Loss = 0.0512\n",
      "Epoch 26/50: Train Loss = 0.0255, Val Loss = 0.0405\n",
      "Epoch 27/50: Train Loss = 0.0247, Val Loss = 0.0465\n",
      "Epoch 28/50: Train Loss = 0.0243, Val Loss = 0.0417\n",
      "Epoch 29/50: Train Loss = 0.0246, Val Loss = 0.0497\n",
      "Epoch 30/50: Train Loss = 0.0252, Val Loss = 0.0486\n",
      "Epoch 31/50: Train Loss = 0.0214, Val Loss = 0.0457\n",
      "Epoch 32/50: Train Loss = 0.0212, Val Loss = 0.0475\n",
      "Epoch 33/50: Train Loss = 0.0215, Val Loss = 0.0455\n",
      "Epoch 34/50: Train Loss = 0.0217, Val Loss = 0.0479\n",
      "Epoch 35/50: Train Loss = 0.0200, Val Loss = 0.0459\n",
      "Epoch 36/50: Train Loss = 0.0198, Val Loss = 0.0447\n",
      "Epoch 37/50: Train Loss = 0.0198, Val Loss = 0.0472\n",
      "Epoch 38/50: Train Loss = 0.0203, Val Loss = 0.0449\n",
      "Epoch 39/50: Train Loss = 0.0190, Val Loss = 0.0471\n",
      "Epoch 40/50: Train Loss = 0.0188, Val Loss = 0.0510\n",
      "Epoch 41/50: Train Loss = 0.0191, Val Loss = 0.0473\n",
      "Epoch 42/50: Train Loss = 0.0185, Val Loss = 0.0519\n",
      "Epoch 43/50: Train Loss = 0.0179, Val Loss = 0.0511\n",
      "Epoch 44/50: Train Loss = 0.0176, Val Loss = 0.0533\n",
      "Epoch 45/50: Train Loss = 0.0179, Val Loss = 0.0557\n",
      "Epoch 46/50: Train Loss = 0.0182, Val Loss = 0.0536\n",
      "Epoch 47/50: Train Loss = 0.0173, Val Loss = 0.0539\n",
      "Epoch 48/50: Train Loss = 0.0175, Val Loss = 0.0569\n",
      "Epoch 49/50: Train Loss = 0.0177, Val Loss = 0.0566\n",
      "Epoch 50/50: Train Loss = 0.0178, Val Loss = 0.0579\n",
      "\n",
      "--- Learning Rate: 0.02 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0955, Val Loss = 0.1318\n",
      "Epoch 2/50: Train Loss = 0.0803, Val Loss = 0.0733\n",
      "Epoch 3/50: Train Loss = 0.0769, Val Loss = 0.0814\n",
      "Epoch 4/50: Train Loss = 0.0721, Val Loss = 0.0654\n",
      "Epoch 5/50: Train Loss = 0.0864, Val Loss = 0.0779\n",
      "Epoch 6/50: Train Loss = 0.0822, Val Loss = 0.0690\n",
      "Epoch 7/50: Train Loss = 0.0768, Val Loss = 0.0682\n",
      "Epoch 8/50: Train Loss = 0.0751, Val Loss = 0.0689\n",
      "Epoch 9/50: Train Loss = 0.0584, Val Loss = 0.0532\n",
      "Epoch 10/50: Train Loss = 0.0555, Val Loss = 0.0561\n",
      "Epoch 11/50: Train Loss = 0.0524, Val Loss = 0.0515\n",
      "Epoch 12/50: Train Loss = 0.0488, Val Loss = 0.0476\n",
      "Epoch 13/50: Train Loss = 0.0481, Val Loss = 0.0551\n",
      "Epoch 14/50: Train Loss = 0.0493, Val Loss = 0.0510\n",
      "Epoch 15/50: Train Loss = 0.0456, Val Loss = 0.0529\n",
      "Epoch 16/50: Train Loss = 0.0429, Val Loss = 0.0472\n",
      "Epoch 17/50: Train Loss = 0.0460, Val Loss = 0.0546\n",
      "Epoch 18/50: Train Loss = 0.0452, Val Loss = 0.0482\n",
      "Epoch 19/50: Train Loss = 0.0480, Val Loss = 0.0551\n",
      "Epoch 20/50: Train Loss = 0.0459, Val Loss = 0.0526\n",
      "Epoch 21/50: Train Loss = 0.0349, Val Loss = 0.0473\n",
      "Epoch 22/50: Train Loss = 0.0376, Val Loss = 0.0472\n",
      "Epoch 23/50: Train Loss = 0.0339, Val Loss = 0.0498\n",
      "Epoch 24/50: Train Loss = 0.0335, Val Loss = 0.0453\n",
      "Epoch 25/50: Train Loss = 0.0309, Val Loss = 0.0455\n",
      "Epoch 26/50: Train Loss = 0.0324, Val Loss = 0.0436\n",
      "Epoch 27/50: Train Loss = 0.0301, Val Loss = 0.0474\n",
      "Epoch 28/50: Train Loss = 0.0308, Val Loss = 0.0659\n",
      "Epoch 29/50: Train Loss = 0.0307, Val Loss = 0.0498\n",
      "Epoch 30/50: Train Loss = 0.0291, Val Loss = 0.0614\n",
      "Epoch 31/50: Train Loss = 0.0246, Val Loss = 0.0428\n",
      "Epoch 32/50: Train Loss = 0.0251, Val Loss = 0.0464\n",
      "Epoch 33/50: Train Loss = 0.0254, Val Loss = 0.0403\n",
      "Epoch 34/50: Train Loss = 0.0245, Val Loss = 0.0450\n",
      "Epoch 35/50: Train Loss = 0.0243, Val Loss = 0.0473\n",
      "Epoch 36/50: Train Loss = 0.0247, Val Loss = 0.0451\n",
      "Epoch 37/50: Train Loss = 0.0241, Val Loss = 0.0440\n",
      "Epoch 38/50: Train Loss = 0.0215, Val Loss = 0.0528\n",
      "Epoch 39/50: Train Loss = 0.0220, Val Loss = 0.0435\n",
      "Epoch 40/50: Train Loss = 0.0215, Val Loss = 0.0460\n",
      "Epoch 41/50: Train Loss = 0.0218, Val Loss = 0.0460\n",
      "Epoch 42/50: Train Loss = 0.0203, Val Loss = 0.0448\n",
      "Epoch 43/50: Train Loss = 0.0198, Val Loss = 0.0473\n",
      "Epoch 44/50: Train Loss = 0.0197, Val Loss = 0.0498\n",
      "Epoch 45/50: Train Loss = 0.0197, Val Loss = 0.0570\n",
      "Epoch 46/50: Train Loss = 0.0189, Val Loss = 0.0487\n",
      "Epoch 47/50: Train Loss = 0.0187, Val Loss = 0.0502\n",
      "Epoch 48/50: Train Loss = 0.0190, Val Loss = 0.0498\n",
      "Epoch 49/50: Train Loss = 0.0194, Val Loss = 0.0497\n",
      "Epoch 50/50: Train Loss = 0.0179, Val Loss = 0.0508\n",
      "\n",
      "--- Learning Rate: 0.05 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.2354, Val Loss = 0.2911\n",
      "Epoch 2/50: Train Loss = 0.1988, Val Loss = 0.1230\n",
      "Epoch 3/50: Train Loss = 0.1758, Val Loss = 0.1358\n",
      "Epoch 4/50: Train Loss = 0.1684, Val Loss = 0.1186\n",
      "Epoch 5/50: Train Loss = 0.2085, Val Loss = 0.1439\n",
      "Epoch 6/50: Train Loss = 0.2235, Val Loss = 0.1404\n",
      "Epoch 7/50: Train Loss = 0.1752, Val Loss = 0.1293\n",
      "Epoch 8/50: Train Loss = 0.1799, Val Loss = 0.1299\n",
      "Epoch 9/50: Train Loss = 0.1388, Val Loss = 0.1044\n",
      "Epoch 10/50: Train Loss = 0.1247, Val Loss = 0.0939\n",
      "Epoch 11/50: Train Loss = 0.1234, Val Loss = 0.0886\n",
      "Epoch 12/50: Train Loss = 0.1099, Val Loss = 0.0876\n",
      "Epoch 13/50: Train Loss = 0.1126, Val Loss = 0.0878\n",
      "Epoch 14/50: Train Loss = 0.1095, Val Loss = 0.1013\n",
      "Epoch 15/50: Train Loss = 0.0997, Val Loss = 0.0752\n",
      "Epoch 16/50: Train Loss = 0.1196, Val Loss = 0.0900\n",
      "Epoch 17/50: Train Loss = 0.1151, Val Loss = 0.0886\n",
      "Epoch 18/50: Train Loss = 0.1152, Val Loss = 0.0896\n",
      "Epoch 19/50: Train Loss = 0.1141, Val Loss = 0.0866\n",
      "Epoch 20/50: Train Loss = 0.0894, Val Loss = 0.0712\n",
      "Epoch 21/50: Train Loss = 0.0857, Val Loss = 0.0757\n",
      "Epoch 22/50: Train Loss = 0.0848, Val Loss = 0.0784\n",
      "Epoch 23/50: Train Loss = 0.0843, Val Loss = 0.0705\n",
      "Epoch 24/50: Train Loss = 0.0830, Val Loss = 0.0894\n",
      "Epoch 25/50: Train Loss = 0.0839, Val Loss = 0.0729\n",
      "Epoch 26/50: Train Loss = 0.0804, Val Loss = 0.0702\n",
      "Epoch 27/50: Train Loss = 0.0824, Val Loss = 0.0772\n",
      "Epoch 28/50: Train Loss = 0.0853, Val Loss = 0.0730\n",
      "Epoch 29/50: Train Loss = 0.0831, Val Loss = 0.0734\n",
      "Epoch 30/50: Train Loss = 0.0835, Val Loss = 0.0779\n",
      "Epoch 31/50: Train Loss = 0.0733, Val Loss = 0.0659\n",
      "Epoch 32/50: Train Loss = 0.0691, Val Loss = 0.0645\n",
      "Epoch 33/50: Train Loss = 0.0684, Val Loss = 0.0655\n",
      "Epoch 34/50: Train Loss = 0.0662, Val Loss = 0.0715\n",
      "Epoch 35/50: Train Loss = 0.0681, Val Loss = 0.0617\n",
      "Epoch 36/50: Train Loss = 0.0653, Val Loss = 0.0601\n",
      "Epoch 37/50: Train Loss = 0.0654, Val Loss = 0.0592\n",
      "Epoch 38/50: Train Loss = 0.0645, Val Loss = 0.0629\n",
      "Epoch 39/50: Train Loss = 0.0633, Val Loss = 0.0586\n",
      "Epoch 40/50: Train Loss = 0.0613, Val Loss = 0.0660\n",
      "Epoch 41/50: Train Loss = 0.0591, Val Loss = 0.0550\n",
      "Epoch 42/50: Train Loss = 0.0587, Val Loss = 0.0543\n",
      "Epoch 43/50: Train Loss = 0.0583, Val Loss = 0.0543\n",
      "Epoch 44/50: Train Loss = 0.0589, Val Loss = 0.0571\n",
      "Epoch 45/50: Train Loss = 0.0610, Val Loss = 0.0561\n",
      "Epoch 46/50: Train Loss = 0.0612, Val Loss = 0.0618\n",
      "Epoch 47/50: Train Loss = 0.0558, Val Loss = 0.0513\n",
      "Epoch 48/50: Train Loss = 0.0497, Val Loss = 0.0579\n",
      "Epoch 49/50: Train Loss = 0.0486, Val Loss = 0.0510\n",
      "Epoch 50/50: Train Loss = 0.0473, Val Loss = 0.0540\n",
      "\n",
      "--- Comprehensive Results ---\n",
      "\n",
      "Learning Rate: 0.001\n",
      "F1 Score: 0.9872649487613173\n",
      "Accuracy: 0.9855056052542180\n",
      "Precision: 0.9925977793338001\n",
      "Recall: 0.9819891142998516\n",
      "\n",
      "Learning Rate: 0.002\n",
      "F1 Score: 0.9872005577967030\n",
      "Accuracy: 0.9854489865247423\n",
      "Precision: 0.9936835773009826\n",
      "Recall: 0.9808015833745670\n",
      "\n",
      "Learning Rate: 0.01\n",
      "F1 Score: 0.9866958991479396\n",
      "Accuracy: 0.9848827992299852\n",
      "Precision: 0.9936772380570053\n",
      "Recall: 0.9798119742701633\n",
      "\n",
      "Learning Rate: 0.02\n",
      "F1 Score: 0.9877835951134381\n",
      "Accuracy: 0.9861284112784510\n",
      "Precision: 0.9954773869346734\n",
      "Recall: 0.9802078179119248\n",
      "\n",
      "Learning Rate: 0.05\n",
      "F1 Score: 0.9854332482354707\n",
      "Accuracy: 0.9835239497225682\n",
      "Precision: 0.9970623987034035\n",
      "Recall: 0.9740722414646215\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n",
    "    demo_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
