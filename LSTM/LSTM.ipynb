{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_len: int = 100):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Convert input to string and clean it\n",
    "        text = str(text)\n",
    "        text = re.sub(r'([<>/=\"])', r' \\1 ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            # Ensure text is string\n",
    "            text = str(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq and word not in self.vocab:\n",
    "                self.vocab[word] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def encode_text(self, text: str) -> List[int]:\n",
    "        # Ensure text is string\n",
    "        text = str(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend(['<PAD>'] * (self.max_len - len(tokens)))\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], preprocessor: TextPreprocessor):\n",
    "        # Convert all texts to strings\n",
    "        self.texts = [str(text) for text in texts]\n",
    "        self.preprocessor = preprocessor\n",
    "        self.encodings = [self.preprocessor.encode_text(text) for text in self.texts]\n",
    "        self.labels = [int(label) for label in labels]  # Convert labels to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "\n",
    "def load_and_clean_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load and clean the dataset, ensuring proper data types.\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert texts to strings and clean them\n",
    "        texts = [str(text).strip() for text in data['Sentence']]\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        labels = [int(label) for label in data['Label']]\n",
    "        \n",
    "        # Basic validation\n",
    "        assert len(texts) == len(labels), \"Number of texts and labels must match\"\n",
    "        assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n",
    "        assert all(isinstance(label, int) and label in [0, 1] for label in labels), \"Labels must be binary (0 or 1)\"\n",
    "        \n",
    "        print(f\"Loaded {len(texts)} samples successfully\")\n",
    "        \n",
    "        # Print some basic statistics\n",
    "        print(f\"Number of positive samples: {sum(labels)}\")\n",
    "        print(f\"Number of negative samples: {len(labels) - sum(labels)}\")\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetectorLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 50, \n",
    "                 hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetector:\n",
    "    def __init__(self, max_len: int = 100, device: str = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else torch.device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.preprocessor = TextPreprocessor(max_len)\n",
    "        self.model = None\n",
    "        self.results = {}\n",
    "\n",
    "    def plot_loss_curves(self):\n",
    "        \"\"\"Plot loss curve visualizations comparing different learning rates.\"\"\"\n",
    "        # Individual plots for each learning rate\n",
    "        for lr in self.results.keys():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            train_losses = self.results[lr]['train_losses']\n",
    "            val_losses = self.results[lr]['val_losses']\n",
    "            epochs = range(1, len(train_losses) + 1)\n",
    "            \n",
    "            plt.plot(epochs, train_losses, label='Training Loss', marker='o', markersize=4)\n",
    "            plt.plot(epochs, val_losses, label='Validation Loss', marker='o', markersize=4)\n",
    "            \n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Training and Validation Loss (Learning Rate: {lr})')\n",
    "            plt.xticks(np.arange(0, len(train_losses) + 1, 5))\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'LSTM_loss_plot_lr_{lr}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Combined training losses plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lr in self.results.keys():\n",
    "            train_losses = self.results[lr]['train_losses']\n",
    "            epochs = range(1, len(train_losses) + 1)\n",
    "            plt.plot(epochs, train_losses, label=f'LR = {lr}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Training Loss Comparison Across Learning Rates')\n",
    "        plt.xticks(np.arange(0, max(len(self.results[lr]['train_losses']) for lr in self.results.keys()) + 1, 5))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('LSTM_combined_training_losses.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Combined validation losses plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lr in self.results.keys():\n",
    "            val_losses = self.results[lr]['val_losses']\n",
    "            epochs = range(1, len(val_losses) + 1)\n",
    "            plt.plot(epochs, val_losses, label=f'LR = {lr}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Validation Loss Comparison Across Learning Rates')\n",
    "        plt.xticks(np.arange(0, max(len(self.results[lr]['val_losses']) for lr in self.results.keys()) + 1, 5))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('LSTM_combined_validation_losses.png')\n",
    "        plt.close()\n",
    "\n",
    "    def train(self, texts: List[str], labels: List[int], \n",
    "              epochs: int = 50, batch_size: int = 16,  \n",
    "              learning_rates: List[float] = [0.001, 0.002, 0.01, 0.02, 0.05]):\n",
    "        try:\n",
    "            texts = [str(text) for text in texts]\n",
    "            labels = torch.tensor(labels, dtype=torch.float)\n",
    "            \n",
    "            self.preprocessor.build_vocab(texts)\n",
    "            dataset = XSSDataset(texts, labels.numpy(), self.preprocessor)\n",
    "            \n",
    "            # Split dataset\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.2 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "            \n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataset splits:\")\n",
    "            print(f\"Training: {train_size} samples\")\n",
    "            print(f\"Validation: {val_size} samples\")\n",
    "            print(f\"Test: {test_size} samples\")\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            # Import metrics calculation functions\n",
    "            from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "            \n",
    "            # Train with different learning rates\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\n--- Learning Rate: {lr} ---\")\n",
    "                \n",
    "                self.model = XSSDetectorLSTM(\n",
    "                    vocab_size=self.preprocessor.vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    dropout=0.3\n",
    "                ).to(self.device)\n",
    "                \n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                criterion = nn.BCELoss()\n",
    "                \n",
    "                # Learning rate scheduler for better convergence\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "                )\n",
    "                \n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training\n",
    "                    self.model.train()\n",
    "                    total_loss = 0\n",
    "                    train_correct = 0\n",
    "                    train_total = 0\n",
    "                    \n",
    "                    for batch_sequences, batch_labels in train_loader:\n",
    "                        batch_sequences = batch_sequences.to(self.device)\n",
    "                        batch_labels = batch_labels.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = self.model(batch_sequences).squeeze()\n",
    "                        loss = criterion(outputs, batch_labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        predictions = (outputs >= 0.5).float()\n",
    "                        train_correct += (predictions == batch_labels).sum().item()\n",
    "                        train_total += len(batch_labels)\n",
    "                    \n",
    "                    avg_train_loss = total_loss / len(train_loader)\n",
    "                    train_losses.append(avg_train_loss)\n",
    "                    \n",
    "                    # Validation\n",
    "                    self.model.eval()\n",
    "                    val_loss = 0\n",
    "                    val_predictions = []\n",
    "                    val_true_labels = []\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_sequences, batch_labels in val_loader:\n",
    "                            batch_sequences = batch_sequences.to(self.device)\n",
    "                            batch_labels = batch_labels.to(self.device)\n",
    "                            \n",
    "                            outputs = self.model(batch_sequences).squeeze()\n",
    "                            batch_val_loss = criterion(outputs, batch_labels).item()\n",
    "                            val_loss += batch_val_loss\n",
    "                            \n",
    "                            predictions = (outputs >= 0.5).float()\n",
    "                            val_predictions.extend(predictions.cpu().numpy())\n",
    "                            val_true_labels.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    \n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "                    \n",
    "                    # Learning rate scheduler step\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        patience_counter = 0\n",
    "                        # Save best model\n",
    "                        torch.save({\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'preprocessor': self.preprocessor\n",
    "                        }, f'LSTM_model_lr_{lr}.pth')\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    if patience_counter > 50:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                # Calculate final metrics\n",
    "                val_predictions = np.array(val_predictions)\n",
    "                val_true_labels = np.array(val_true_labels)\n",
    "                \n",
    "                f1 = f1_score(val_true_labels, val_predictions)\n",
    "                accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "                precision = precision_score(val_true_labels, val_predictions)\n",
    "                recall = recall_score(val_true_labels, val_predictions)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[lr] = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'f1_score': f1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall\n",
    "                }\n",
    "            \n",
    "            # Print comprehensive results\n",
    "            print(\"\\n--- Comprehensive Results ---\")\n",
    "            for lr in learning_rates:\n",
    "                r = self.results[lr]\n",
    "                print(f\"\\nLearning Rate: {lr}\")\n",
    "                print(f\"F1 Score: {r['f1_score']:.16f}\")\n",
    "                print(f\"Accuracy: {r['accuracy']:.16f}\")\n",
    "                print(f\"Precision: {r['precision']:.16f}\")\n",
    "                print(f\"Recall: {r['recall']:.16f}\")\n",
    "            \n",
    "            # Plot all loss curves\n",
    "            self.plot_loss_curves()\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Training error: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-execution environment check\n",
    "def check_environment():\n",
    "    print(\"\\n--- Environment Check ---\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    try:\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    except:\n",
    "        print(\"No CUDA device currently selected\")\n",
    "\n",
    "def demo_detector(dataset_path='../Training Dataset/final_dataset.csv'):\n",
    "    print(\"\\n--- XSS Detection Model Demonstration ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load and clean data\n",
    "        texts, labels = load_and_clean_data(dataset_path)\n",
    "        \n",
    "        # Initialize and train detector\n",
    "        detector = XSSDetector(max_len=100)\n",
    "        detector.train(\n",
    "            texts=texts,\n",
    "            labels=labels,\n",
    "            epochs=50,\n",
    "            batch_size=16,\n",
    "            learning_rates=[0.001, 0.002, 0.01, 0.02, 0.05]\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Demonstration failed: {e}\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. Ensure correct dataset path\")\n",
    "        print(\"2. Check dataset format\")\n",
    "        print(\"3. Verify required libraries are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Environment Check ---\n",
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "\n",
      "--- XSS Detection Model Demonstration ---\n",
      "Loaded 88310 samples successfully\n",
      "Number of positive samples: 50590\n",
      "Number of negative samples: 37720\n",
      "Using device: cuda\n",
      "\n",
      "Dataset splits:\n",
      "Training: 61816 samples\n",
      "Validation: 17662 samples\n",
      "Test: 8832 samples\n",
      "\n",
      "--- Learning Rate: 0.001 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0833, Val Loss = 0.0559\n",
      "Epoch 2/50: Train Loss = 0.0448, Val Loss = 0.0511\n",
      "Epoch 3/50: Train Loss = 0.0349, Val Loss = 0.0366\n",
      "Epoch 4/50: Train Loss = 0.0290, Val Loss = 0.0395\n",
      "Epoch 5/50: Train Loss = 0.0280, Val Loss = 0.0410\n",
      "Epoch 6/50: Train Loss = 0.0273, Val Loss = 0.0385\n",
      "Epoch 7/50: Train Loss = 0.0270, Val Loss = 0.0406\n",
      "Epoch 8/50: Train Loss = 0.0227, Val Loss = 0.0391\n",
      "Epoch 9/50: Train Loss = 0.0224, Val Loss = 0.0388\n",
      "Epoch 10/50: Train Loss = 0.0251, Val Loss = 0.0391\n",
      "Epoch 11/50: Train Loss = 0.0233, Val Loss = 0.0367\n",
      "Epoch 12/50: Train Loss = 0.0203, Val Loss = 0.0424\n",
      "Epoch 13/50: Train Loss = 0.0198, Val Loss = 0.0444\n",
      "Epoch 14/50: Train Loss = 0.0206, Val Loss = 0.0418\n",
      "Epoch 15/50: Train Loss = 0.0198, Val Loss = 0.0408\n",
      "Epoch 16/50: Train Loss = 0.0185, Val Loss = 0.0406\n",
      "Epoch 17/50: Train Loss = 0.0181, Val Loss = 0.0430\n",
      "Epoch 18/50: Train Loss = 0.0191, Val Loss = 0.0439\n",
      "Epoch 19/50: Train Loss = 0.0188, Val Loss = 0.0446\n",
      "Epoch 20/50: Train Loss = 0.0175, Val Loss = 0.0455\n",
      "Epoch 21/50: Train Loss = 0.0174, Val Loss = 0.0497\n",
      "Epoch 22/50: Train Loss = 0.0174, Val Loss = 0.0498\n",
      "Epoch 23/50: Train Loss = 0.0174, Val Loss = 0.0495\n",
      "Epoch 24/50: Train Loss = 0.0171, Val Loss = 0.0490\n",
      "Epoch 25/50: Train Loss = 0.0168, Val Loss = 0.0505\n",
      "Epoch 26/50: Train Loss = 0.0173, Val Loss = 0.0497\n",
      "Epoch 27/50: Train Loss = 0.0168, Val Loss = 0.0505\n",
      "Epoch 28/50: Train Loss = 0.0163, Val Loss = 0.0513\n",
      "Epoch 29/50: Train Loss = 0.0165, Val Loss = 0.0531\n",
      "Epoch 30/50: Train Loss = 0.0170, Val Loss = 0.0528\n",
      "Epoch 31/50: Train Loss = 0.0166, Val Loss = 0.0519\n",
      "Epoch 32/50: Train Loss = 0.0166, Val Loss = 0.0513\n",
      "Epoch 33/50: Train Loss = 0.0166, Val Loss = 0.0517\n",
      "Epoch 34/50: Train Loss = 0.0163, Val Loss = 0.0518\n",
      "Epoch 35/50: Train Loss = 0.0165, Val Loss = 0.0521\n",
      "Epoch 36/50: Train Loss = 0.0168, Val Loss = 0.0526\n",
      "Epoch 37/50: Train Loss = 0.0162, Val Loss = 0.0533\n",
      "Epoch 38/50: Train Loss = 0.0165, Val Loss = 0.0528\n",
      "Epoch 39/50: Train Loss = 0.0162, Val Loss = 0.0532\n",
      "Epoch 40/50: Train Loss = 0.0163, Val Loss = 0.0532\n",
      "Epoch 41/50: Train Loss = 0.0160, Val Loss = 0.0534\n",
      "Epoch 42/50: Train Loss = 0.0163, Val Loss = 0.0534\n",
      "Epoch 43/50: Train Loss = 0.0162, Val Loss = 0.0534\n",
      "Epoch 44/50: Train Loss = 0.0161, Val Loss = 0.0535\n",
      "Epoch 45/50: Train Loss = 0.0162, Val Loss = 0.0536\n",
      "Epoch 46/50: Train Loss = 0.0163, Val Loss = 0.0537\n",
      "Epoch 47/50: Train Loss = 0.0163, Val Loss = 0.0536\n",
      "Epoch 48/50: Train Loss = 0.0162, Val Loss = 0.0537\n",
      "Epoch 49/50: Train Loss = 0.0161, Val Loss = 0.0536\n",
      "Epoch 50/50: Train Loss = 0.0160, Val Loss = 0.0537\n",
      "\n",
      "--- Learning Rate: 0.002 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0748, Val Loss = 0.0443\n",
      "Epoch 2/50: Train Loss = 0.0392, Val Loss = 0.0381\n",
      "Epoch 3/50: Train Loss = 0.0319, Val Loss = 0.0415\n",
      "Epoch 4/50: Train Loss = 0.0308, Val Loss = 0.0425\n",
      "Epoch 5/50: Train Loss = 0.0298, Val Loss = 0.0391\n",
      "Epoch 6/50: Train Loss = 0.0281, Val Loss = 0.0373\n",
      "Epoch 7/50: Train Loss = 0.0332, Val Loss = 0.0367\n",
      "Epoch 8/50: Train Loss = 0.0274, Val Loss = 0.0429\n",
      "Epoch 9/50: Train Loss = 0.0276, Val Loss = 0.0390\n",
      "Epoch 10/50: Train Loss = 0.0264, Val Loss = 0.0392\n",
      "Epoch 11/50: Train Loss = 0.0272, Val Loss = 0.0364\n",
      "Epoch 12/50: Train Loss = 0.0261, Val Loss = 0.0380\n",
      "Epoch 13/50: Train Loss = 0.0254, Val Loss = 0.0428\n",
      "Epoch 14/50: Train Loss = 0.0298, Val Loss = 0.0413\n",
      "Epoch 15/50: Train Loss = 0.0252, Val Loss = 0.0367\n",
      "Epoch 16/50: Train Loss = 0.0220, Val Loss = 0.0374\n",
      "Epoch 17/50: Train Loss = 0.0217, Val Loss = 0.0386\n",
      "Epoch 18/50: Train Loss = 0.0224, Val Loss = 0.0389\n",
      "Epoch 19/50: Train Loss = 0.0219, Val Loss = 0.0382\n",
      "Epoch 20/50: Train Loss = 0.0197, Val Loss = 0.0379\n",
      "Epoch 21/50: Train Loss = 0.0194, Val Loss = 0.0407\n",
      "Epoch 22/50: Train Loss = 0.0199, Val Loss = 0.0385\n",
      "Epoch 23/50: Train Loss = 0.0201, Val Loss = 0.0396\n",
      "Epoch 24/50: Train Loss = 0.0181, Val Loss = 0.0420\n",
      "Epoch 25/50: Train Loss = 0.0186, Val Loss = 0.0405\n",
      "Epoch 26/50: Train Loss = 0.0186, Val Loss = 0.0426\n",
      "Epoch 27/50: Train Loss = 0.0184, Val Loss = 0.0419\n",
      "Epoch 28/50: Train Loss = 0.0176, Val Loss = 0.0418\n",
      "Epoch 29/50: Train Loss = 0.0176, Val Loss = 0.0420\n",
      "Epoch 30/50: Train Loss = 0.0177, Val Loss = 0.0425\n",
      "Epoch 31/50: Train Loss = 0.0175, Val Loss = 0.0412\n",
      "Epoch 32/50: Train Loss = 0.0165, Val Loss = 0.0434\n",
      "Epoch 33/50: Train Loss = 0.0166, Val Loss = 0.0438\n",
      "Epoch 34/50: Train Loss = 0.0167, Val Loss = 0.0452\n",
      "Epoch 35/50: Train Loss = 0.0169, Val Loss = 0.0440\n",
      "Epoch 36/50: Train Loss = 0.0167, Val Loss = 0.0453\n",
      "Epoch 37/50: Train Loss = 0.0165, Val Loss = 0.0466\n",
      "Epoch 38/50: Train Loss = 0.0168, Val Loss = 0.0464\n",
      "Epoch 39/50: Train Loss = 0.0163, Val Loss = 0.0487\n",
      "Epoch 40/50: Train Loss = 0.0164, Val Loss = 0.0486\n",
      "Epoch 41/50: Train Loss = 0.0164, Val Loss = 0.0479\n",
      "Epoch 42/50: Train Loss = 0.0160, Val Loss = 0.0481\n",
      "Epoch 43/50: Train Loss = 0.0163, Val Loss = 0.0492\n",
      "Epoch 44/50: Train Loss = 0.0159, Val Loss = 0.0492\n",
      "Epoch 45/50: Train Loss = 0.0166, Val Loss = 0.0491\n",
      "Epoch 46/50: Train Loss = 0.0162, Val Loss = 0.0488\n",
      "Epoch 47/50: Train Loss = 0.0161, Val Loss = 0.0489\n",
      "Epoch 48/50: Train Loss = 0.0163, Val Loss = 0.0491\n",
      "Epoch 49/50: Train Loss = 0.0163, Val Loss = 0.0495\n",
      "Epoch 50/50: Train Loss = 0.0162, Val Loss = 0.0495\n",
      "\n",
      "--- Learning Rate: 0.01 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0836, Val Loss = 0.0573\n",
      "Epoch 2/50: Train Loss = 0.0535, Val Loss = 0.0522\n",
      "Epoch 3/50: Train Loss = 0.0575, Val Loss = 0.0586\n",
      "Epoch 4/50: Train Loss = 0.0532, Val Loss = 0.0487\n",
      "Epoch 5/50: Train Loss = 0.0519, Val Loss = 0.0489\n",
      "Epoch 6/50: Train Loss = 0.0509, Val Loss = 0.0588\n",
      "Epoch 7/50: Train Loss = 0.0533, Val Loss = 0.0503\n",
      "Epoch 8/50: Train Loss = 0.0461, Val Loss = 0.0446\n",
      "Epoch 9/50: Train Loss = 0.0501, Val Loss = 0.0568\n",
      "Epoch 10/50: Train Loss = 0.0475, Val Loss = 0.0629\n",
      "Epoch 11/50: Train Loss = 0.0511, Val Loss = 0.0467\n",
      "Epoch 12/50: Train Loss = 0.0497, Val Loss = 0.0581\n",
      "Epoch 13/50: Train Loss = 0.0398, Val Loss = 0.0473\n",
      "Epoch 14/50: Train Loss = 0.0386, Val Loss = 0.0565\n",
      "Epoch 15/50: Train Loss = 0.0383, Val Loss = 0.0446\n",
      "Epoch 16/50: Train Loss = 0.0348, Val Loss = 0.0440\n",
      "Epoch 17/50: Train Loss = 0.0348, Val Loss = 0.0491\n",
      "Epoch 18/50: Train Loss = 0.0353, Val Loss = 0.0409\n",
      "Epoch 19/50: Train Loss = 0.0345, Val Loss = 0.0381\n",
      "Epoch 20/50: Train Loss = 0.0333, Val Loss = 0.0413\n",
      "Epoch 21/50: Train Loss = 0.0324, Val Loss = 0.0460\n",
      "Epoch 22/50: Train Loss = 0.0327, Val Loss = 0.0399\n",
      "Epoch 23/50: Train Loss = 0.0320, Val Loss = 0.0380\n",
      "Epoch 24/50: Train Loss = 0.0320, Val Loss = 0.0437\n",
      "Epoch 25/50: Train Loss = 0.0315, Val Loss = 0.0381\n",
      "Epoch 26/50: Train Loss = 0.0316, Val Loss = 0.0397\n",
      "Epoch 27/50: Train Loss = 0.0303, Val Loss = 0.0456\n",
      "Epoch 28/50: Train Loss = 0.0263, Val Loss = 0.0406\n",
      "Epoch 29/50: Train Loss = 0.0272, Val Loss = 0.0392\n",
      "Epoch 30/50: Train Loss = 0.0263, Val Loss = 0.0386\n",
      "Epoch 31/50: Train Loss = 0.0268, Val Loss = 0.0373\n",
      "Epoch 32/50: Train Loss = 0.0257, Val Loss = 0.0393\n",
      "Epoch 33/50: Train Loss = 0.0256, Val Loss = 0.0472\n",
      "Epoch 34/50: Train Loss = 0.0256, Val Loss = 0.0373\n",
      "Epoch 35/50: Train Loss = 0.0249, Val Loss = 0.0450\n",
      "Epoch 36/50: Train Loss = 0.0224, Val Loss = 0.0408\n",
      "Epoch 37/50: Train Loss = 0.0229, Val Loss = 0.0381\n",
      "Epoch 38/50: Train Loss = 0.0228, Val Loss = 0.0379\n",
      "Epoch 39/50: Train Loss = 0.0229, Val Loss = 0.0381\n",
      "Epoch 40/50: Train Loss = 0.0201, Val Loss = 0.0400\n",
      "Epoch 41/50: Train Loss = 0.0203, Val Loss = 0.0388\n",
      "Epoch 42/50: Train Loss = 0.0205, Val Loss = 0.0391\n",
      "Epoch 43/50: Train Loss = 0.0206, Val Loss = 0.0399\n",
      "Epoch 44/50: Train Loss = 0.0189, Val Loss = 0.0448\n",
      "Epoch 45/50: Train Loss = 0.0187, Val Loss = 0.0449\n",
      "Epoch 46/50: Train Loss = 0.0189, Val Loss = 0.0450\n",
      "Epoch 47/50: Train Loss = 0.0183, Val Loss = 0.0439\n",
      "Epoch 48/50: Train Loss = 0.0179, Val Loss = 0.0427\n",
      "Epoch 49/50: Train Loss = 0.0177, Val Loss = 0.0483\n",
      "Epoch 50/50: Train Loss = 0.0175, Val Loss = 0.0462\n",
      "\n",
      "--- Learning Rate: 0.02 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.1004, Val Loss = 0.0929\n",
      "Epoch 2/50: Train Loss = 0.0857, Val Loss = 0.0676\n",
      "Epoch 3/50: Train Loss = 0.0847, Val Loss = 0.0728\n",
      "Epoch 4/50: Train Loss = 0.0798, Val Loss = 0.0759\n",
      "Epoch 5/50: Train Loss = 0.0789, Val Loss = 0.0666\n",
      "Epoch 6/50: Train Loss = 0.0798, Val Loss = 0.0730\n",
      "Epoch 7/50: Train Loss = 0.0769, Val Loss = 0.0663\n",
      "Epoch 8/50: Train Loss = 0.0804, Val Loss = 0.0808\n",
      "Epoch 9/50: Train Loss = 0.0820, Val Loss = 0.0732\n",
      "Epoch 10/50: Train Loss = 0.0879, Val Loss = 0.0753\n",
      "Epoch 11/50: Train Loss = 0.0859, Val Loss = 0.0767\n",
      "Epoch 12/50: Train Loss = 0.0645, Val Loss = 0.0612\n",
      "Epoch 13/50: Train Loss = 0.0635, Val Loss = 0.0700\n",
      "Epoch 14/50: Train Loss = 0.0566, Val Loss = 0.0628\n",
      "Epoch 15/50: Train Loss = 0.0552, Val Loss = 0.0551\n",
      "Epoch 16/50: Train Loss = 0.0544, Val Loss = 0.0501\n",
      "Epoch 17/50: Train Loss = 0.0512, Val Loss = 0.0485\n",
      "Epoch 18/50: Train Loss = 0.0510, Val Loss = 0.0557\n",
      "Epoch 19/50: Train Loss = 0.0504, Val Loss = 0.0500\n",
      "Epoch 20/50: Train Loss = 0.0472, Val Loss = 0.0496\n",
      "Epoch 21/50: Train Loss = 0.0496, Val Loss = 0.0486\n",
      "Epoch 22/50: Train Loss = 0.0357, Val Loss = 0.0457\n",
      "Epoch 23/50: Train Loss = 0.0343, Val Loss = 0.0481\n",
      "Epoch 24/50: Train Loss = 0.0347, Val Loss = 0.0401\n",
      "Epoch 25/50: Train Loss = 0.0330, Val Loss = 0.0396\n",
      "Epoch 26/50: Train Loss = 0.0317, Val Loss = 0.0410\n",
      "Epoch 27/50: Train Loss = 0.0319, Val Loss = 0.0410\n",
      "Epoch 28/50: Train Loss = 0.0355, Val Loss = 0.0409\n",
      "Epoch 29/50: Train Loss = 0.0324, Val Loss = 0.0429\n",
      "Epoch 30/50: Train Loss = 0.0267, Val Loss = 0.0441\n",
      "Epoch 31/50: Train Loss = 0.0272, Val Loss = 0.0468\n",
      "Epoch 32/50: Train Loss = 0.0266, Val Loss = 0.0370\n",
      "Epoch 33/50: Train Loss = 0.0264, Val Loss = 0.0382\n",
      "Epoch 34/50: Train Loss = 0.0257, Val Loss = 0.0399\n",
      "Epoch 35/50: Train Loss = 0.0253, Val Loss = 0.0425\n",
      "Epoch 36/50: Train Loss = 0.0251, Val Loss = 0.0395\n",
      "Epoch 37/50: Train Loss = 0.0227, Val Loss = 0.0406\n",
      "Epoch 38/50: Train Loss = 0.0224, Val Loss = 0.0410\n",
      "Epoch 39/50: Train Loss = 0.0218, Val Loss = 0.0425\n",
      "Epoch 40/50: Train Loss = 0.0226, Val Loss = 0.0438\n",
      "Epoch 41/50: Train Loss = 0.0200, Val Loss = 0.0431\n",
      "Epoch 42/50: Train Loss = 0.0198, Val Loss = 0.0494\n",
      "Epoch 43/50: Train Loss = 0.0208, Val Loss = 0.0432\n",
      "Epoch 44/50: Train Loss = 0.0206, Val Loss = 0.0427\n",
      "Epoch 45/50: Train Loss = 0.0190, Val Loss = 0.0403\n",
      "Epoch 46/50: Train Loss = 0.0185, Val Loss = 0.0418\n",
      "Epoch 47/50: Train Loss = 0.0186, Val Loss = 0.0419\n",
      "Epoch 48/50: Train Loss = 0.0189, Val Loss = 0.0455\n",
      "Epoch 49/50: Train Loss = 0.0179, Val Loss = 0.0435\n",
      "Epoch 50/50: Train Loss = 0.0178, Val Loss = 0.0452\n",
      "\n",
      "--- Learning Rate: 0.05 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.3076, Val Loss = 0.4284\n",
      "Epoch 2/50: Train Loss = 0.4176, Val Loss = 0.3084\n",
      "Epoch 3/50: Train Loss = 0.5023, Val Loss = 0.5177\n",
      "Epoch 4/50: Train Loss = 0.5230, Val Loss = 0.4919\n",
      "Epoch 5/50: Train Loss = 0.5208, Val Loss = 0.5193\n",
      "Epoch 6/50: Train Loss = 0.5474, Val Loss = 0.4893\n",
      "Epoch 7/50: Train Loss = 0.5225, Val Loss = 0.5356\n",
      "Epoch 8/50: Train Loss = 0.5114, Val Loss = 0.5157\n",
      "Epoch 9/50: Train Loss = 0.4886, Val Loss = 0.4647\n",
      "Epoch 10/50: Train Loss = 0.5105, Val Loss = 0.4874\n",
      "Epoch 11/50: Train Loss = 0.5084, Val Loss = 0.4746\n",
      "Epoch 12/50: Train Loss = 0.5100, Val Loss = 0.4517\n",
      "Epoch 13/50: Train Loss = 0.5077, Val Loss = 0.4690\n",
      "Epoch 14/50: Train Loss = 0.5028, Val Loss = 0.4354\n",
      "Epoch 15/50: Train Loss = 0.4860, Val Loss = 0.4227\n",
      "Epoch 16/50: Train Loss = 0.4669, Val Loss = 0.3656\n",
      "Epoch 17/50: Train Loss = 0.4590, Val Loss = 0.3926\n",
      "Epoch 18/50: Train Loss = 0.4616, Val Loss = 0.3838\n",
      "Epoch 19/50: Train Loss = 0.4522, Val Loss = 0.3975\n",
      "Epoch 20/50: Train Loss = 0.4526, Val Loss = 0.3870\n",
      "Epoch 21/50: Train Loss = 0.4538, Val Loss = 0.3905\n",
      "Epoch 22/50: Train Loss = 0.4505, Val Loss = 0.3819\n",
      "Epoch 23/50: Train Loss = 0.4515, Val Loss = 0.3752\n",
      "Epoch 24/50: Train Loss = 0.4480, Val Loss = 0.3808\n",
      "Epoch 25/50: Train Loss = 0.4480, Val Loss = 0.3792\n",
      "Epoch 26/50: Train Loss = 0.4507, Val Loss = 0.3686\n",
      "Epoch 27/50: Train Loss = 0.4477, Val Loss = 0.3747\n",
      "Epoch 28/50: Train Loss = 0.4459, Val Loss = 0.3659\n",
      "Epoch 29/50: Train Loss = 0.4492, Val Loss = 0.3647\n",
      "Epoch 30/50: Train Loss = 0.4474, Val Loss = 0.3675\n",
      "Epoch 31/50: Train Loss = 0.4420, Val Loss = 0.3672\n",
      "Epoch 32/50: Train Loss = 0.4454, Val Loss = 0.3679\n",
      "Epoch 33/50: Train Loss = 0.4421, Val Loss = 0.3633\n",
      "Epoch 34/50: Train Loss = 0.4458, Val Loss = 0.3620\n",
      "Epoch 35/50: Train Loss = 0.4402, Val Loss = 0.3618\n",
      "Epoch 36/50: Train Loss = 0.4411, Val Loss = 0.3607\n",
      "Epoch 37/50: Train Loss = 0.4407, Val Loss = 0.3612\n",
      "Epoch 38/50: Train Loss = 0.4389, Val Loss = 0.3636\n",
      "Epoch 39/50: Train Loss = 0.4386, Val Loss = 0.3604\n",
      "Epoch 40/50: Train Loss = 0.4420, Val Loss = 0.3667\n",
      "Epoch 41/50: Train Loss = 0.4360, Val Loss = 0.3647\n",
      "Epoch 42/50: Train Loss = 0.4388, Val Loss = 0.3593\n",
      "Epoch 43/50: Train Loss = 0.4416, Val Loss = 0.3628\n",
      "Epoch 44/50: Train Loss = 0.4407, Val Loss = 0.3621\n",
      "Epoch 45/50: Train Loss = 0.4393, Val Loss = 0.3634\n",
      "Epoch 46/50: Train Loss = 0.4374, Val Loss = 0.3600\n",
      "Epoch 47/50: Train Loss = 0.4388, Val Loss = 0.3595\n",
      "Epoch 48/50: Train Loss = 0.4387, Val Loss = 0.3602\n",
      "Epoch 49/50: Train Loss = 0.4354, Val Loss = 0.3591\n",
      "Epoch 50/50: Train Loss = 0.4378, Val Loss = 0.3576\n",
      "\n",
      "--- Comprehensive Results ---\n",
      "\n",
      "Learning Rate: 0.001\n",
      "F1 Score: 0.9879375123591062\n",
      "Accuracy: 0.9861850300079267\n",
      "Precision: 0.9928457869634341\n",
      "Recall: 0.9830775285320740\n",
      "\n",
      "Learning Rate: 0.002\n",
      "F1 Score: 0.9894122303582030\n",
      "Accuracy: 0.9878835918921980\n",
      "Precision: 0.9951234076433121\n",
      "Recall: 0.9837662337662337\n",
      "\n",
      "Learning Rate: 0.01\n",
      "F1 Score: 0.9883876068587241\n",
      "Accuracy: 0.9866945985732081\n",
      "Precision: 0.9928521790926238\n",
      "Recall: 0.9839630066902794\n",
      "\n",
      "Learning Rate: 0.02\n",
      "F1 Score: 0.9888795532051599\n",
      "Accuracy: 0.9872607858679652\n",
      "Precision: 0.9935445426556758\n",
      "Recall: 0.9842581660763479\n",
      "\n",
      "Learning Rate: 0.05\n",
      "F1 Score: 0.8739920797747984\n",
      "Accuracy: 0.8504699354546484\n",
      "Precision: 0.8484483557202408\n",
      "Recall: 0.9011216056670602\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n",
    "    demo_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
