{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_len: int = 100):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Convert input to string and clean it\n",
    "        text = str(text)\n",
    "        text = re.sub(r'([<>/=\"])', r' \\1 ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            # Ensure text is string\n",
    "            text = str(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq and word not in self.vocab:\n",
    "                self.vocab[word] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def encode_text(self, text: str) -> List[int]:\n",
    "        # Ensure text is string\n",
    "        text = str(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend(['<PAD>'] * (self.max_len - len(tokens)))\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], preprocessor: TextPreprocessor):\n",
    "        # Convert all texts to strings\n",
    "        self.texts = [str(text) for text in texts]\n",
    "        self.preprocessor = preprocessor\n",
    "        self.encodings = [self.preprocessor.encode_text(text) for text in self.texts]\n",
    "        self.labels = [int(label) for label in labels]  # Convert labels to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "\n",
    "def load_and_clean_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load and clean the dataset, ensuring proper data types.\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert texts to strings and clean them\n",
    "        texts = [str(text).strip() for text in data['Sentence']]\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        labels = [int(label) for label in data['Label']]\n",
    "        \n",
    "        # Basic validation\n",
    "        assert len(texts) == len(labels), \"Number of texts and labels must match\"\n",
    "        assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n",
    "        assert all(isinstance(label, int) and label in [0, 1] for label in labels), \"Labels must be binary (0 or 1)\"\n",
    "        \n",
    "        print(f\"Loaded {len(texts)} samples successfully\")\n",
    "        \n",
    "        # Print some basic statistics\n",
    "        print(f\"Number of positive samples: {sum(labels)}\")\n",
    "        print(f\"Number of negative samples: {len(labels) - sum(labels)}\")\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetectorLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 50, \n",
    "                 hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetector:\n",
    "    def __init__(self, max_len: int = 100, device: str = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else torch.device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.preprocessor = TextPreprocessor(max_len)\n",
    "        self.model = None\n",
    "        self.results = {}\n",
    "    \n",
    "    def train(self, texts: List[str], labels: List[int], \n",
    "              epochs: int = 20, batch_size: int = 16,  \n",
    "              learning_rates: List[float] = [0.001, 0.002, 0.01, 0.02, 0.05]):\n",
    "        \n",
    "        try:\n",
    "            texts = [str(text) for text in texts]\n",
    "            labels = torch.tensor(labels, dtype=torch.float)\n",
    "            \n",
    "            self.preprocessor.build_vocab(texts)\n",
    "            dataset = XSSDataset(texts, labels.numpy(), self.preprocessor)\n",
    "            \n",
    "            # Split dataset\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.2 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "            \n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataset splits:\")\n",
    "            print(f\"Training: {train_size} samples\")\n",
    "            print(f\"Validation: {val_size} samples\")\n",
    "            print(f\"Test: {test_size} samples\")\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            # Train with different learning rates\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\nTraining with learning rate: {lr}\")\n",
    "                print(f\"Current learning rate: {lr}\")\n",
    "                \n",
    "                self.model = XSSDetectorLSTM(\n",
    "                    vocab_size=self.preprocessor.vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    dropout=0.3  # Increased dropout for regularization\n",
    "                ).to(self.device)\n",
    "                \n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                criterion = nn.BCELoss()\n",
    "                \n",
    "                # Learning rate scheduler for better convergence\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "                )\n",
    "                \n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training\n",
    "                    self.model.train()\n",
    "                    total_loss = 0\n",
    "                    train_correct = 0\n",
    "                    train_total = 0\n",
    "                    \n",
    "                    for batch_sequences, batch_labels in train_loader:\n",
    "                        batch_sequences = batch_sequences.to(self.device)\n",
    "                        batch_labels = batch_labels.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = self.model(batch_sequences).squeeze()\n",
    "                        loss = criterion(outputs, batch_labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        predictions = (outputs >= 0.5).float()\n",
    "                        train_correct += (predictions == batch_labels).sum().item()\n",
    "                        train_total += len(batch_labels)\n",
    "                    \n",
    "                    avg_train_loss = total_loss / len(train_loader)\n",
    "                    train_losses.append(avg_train_loss)\n",
    "                    \n",
    "                    # Validation\n",
    "                    self.model.eval()\n",
    "                    val_loss = 0\n",
    "                    val_correct = 0\n",
    "                    val_total = 0\n",
    "                    val_predictions = []\n",
    "                    val_true_labels = []\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_sequences, batch_labels in val_loader:\n",
    "                            batch_sequences = batch_sequences.to(self.device)\n",
    "                            batch_labels = batch_labels.to(self.device)\n",
    "                            \n",
    "                            outputs = self.model(batch_sequences).squeeze()\n",
    "                            batch_val_loss = criterion(outputs, batch_labels).item()\n",
    "                            val_loss += batch_val_loss\n",
    "                            \n",
    "                            predictions = (outputs >= 0.5).float()\n",
    "                            val_correct += (predictions == batch_labels).sum().item()\n",
    "                            val_total += len(batch_labels)\n",
    "                            \n",
    "                            val_predictions.extend(predictions.cpu().numpy())\n",
    "                            val_true_labels.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    \n",
    "                    # Learning rate scheduler step with current validation loss\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch+1}, Current LR: {current_lr:.6f}\")\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        patience_counter = 0\n",
    "                        # Save the best model\n",
    "                        torch.save({\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'preprocessor': self.preprocessor\n",
    "                        }, f'best_model_lr_{lr}.pth')\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    if patience_counter > 5:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                    \n",
    "                    if (epoch + 1) % 5 == 0:\n",
    "                        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "                        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "                        print(f'Training Accuracy: {100*train_correct/train_total:.2f}%')\n",
    "                        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "                        print(f'Validation Accuracy: {100*val_correct/val_total:.2f}%')\n",
    "                \n",
    "                # Final evaluation\n",
    "                val_f1 = f1_score(val_true_labels, val_predictions)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[lr] = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'final_train_loss': avg_train_loss,\n",
    "                    'final_val_loss': avg_val_loss,\n",
    "                    'train_accuracy': 100 * train_correct / train_total,\n",
    "                    'val_accuracy': 100 * val_correct / val_total,\n",
    "                    'f1_score': val_f1\n",
    "                }\n",
    "                \n",
    "                # Plot normalized training curves\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(range(1, len(train_losses) + 1), \n",
    "                         [loss/train_losses[0] for loss in train_losses], \n",
    "                         label='Normalized Training Loss')\n",
    "                plt.plot(range(1, len(val_losses) + 1), \n",
    "                         [loss/val_losses[0] for loss in val_losses], \n",
    "                         label='Normalized Validation Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss (Normalized to First Epoch)')\n",
    "                plt.title(f'Normalized Training and Validation Loss (Learning Rate: {lr})')\n",
    "                plt.ylim(0, 8)  # Allow variation between 0 and 8 times the initial loss\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'normalized_loss_plot_lr_{lr}.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # Print final results\n",
    "            print(\"\\nFinal Results for All Learning Rates:\")\n",
    "            print(\"\\nLR      Train Loss  Val Loss    Train Acc   Val Acc    F1 Score\")\n",
    "            print(\"-\" * 65)\n",
    "            for lr in learning_rates:\n",
    "                r = self.results[lr]\n",
    "                print(f\"{lr:.3f}  {r['final_train_loss']:.4f}     {r['final_val_loss']:.4f}     \"\n",
    "                      f\"{r['train_accuracy']:.2f}%     {r['val_accuracy']:.2f}%     {r['f1_score']:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Training error: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-execution environment check\n",
    "def check_environment():\n",
    "    print(\"\\n--- Environment Check ---\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    try:\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    except:\n",
    "        print(\"No CUDA device currently selected\")\n",
    "\n",
    "def demo_detector(dataset_path='Training Dataset/final_dataset.csv'):\n",
    "    print(\"\\n--- XSS Detection Model Demonstration ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load and clean data\n",
    "        texts, labels = load_and_clean_data(dataset_path)\n",
    "        \n",
    "        # Initialize and train detector\n",
    "        detector = XSSDetector(max_len=100)\n",
    "        detector.train(\n",
    "            texts=texts,\n",
    "            labels=labels,\n",
    "            epochs=20,\n",
    "            batch_size=16,\n",
    "            learning_rates=[0.001, 0.002, 0.01, 0.02, 0.05]\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Demonstration failed: {e}\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. Ensure correct dataset path\")\n",
    "        print(\"2. Check dataset format\")\n",
    "        print(\"3. Verify required libraries are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Environment Check ---\n",
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "\n",
      "--- XSS Detection Model Demonstration ---\n",
      "Loaded 88310 samples successfully\n",
      "Number of positive samples: 50590\n",
      "Number of negative samples: 37720\n",
      "Using device: cuda\n",
      "\n",
      "Dataset splits:\n",
      "Training: 61816 samples\n",
      "Validation: 17662 samples\n",
      "Test: 8832 samples\n",
      "\n",
      "Training with learning rate: 0.001\n",
      "Current learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Current LR: 0.001000\n",
      "Epoch 2, Current LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     check_environment()\n\u001b[0;32m----> 3\u001b[0m     demo_detector()\n",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m, in \u001b[0;36mdemo_detector\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Initialize and train detector\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     detector \u001b[38;5;241m=\u001b[39m XSSDetector(max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     detector\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     22\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m     23\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m     24\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     25\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     26\u001b[0m         learning_rates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.002\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.02\u001b[39m, \u001b[38;5;241m0.05\u001b[39m]\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDemonstration failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 106\u001b[0m, in \u001b[0;36mXSSDetector.train\u001b[0;34m(self, texts, labels, epochs, batch_size, learning_rates)\u001b[0m\n\u001b[1;32m    103\u001b[0m batch_val_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    104\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_val_loss\n\u001b[0;32m--> 106\u001b[0m predictions \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    107\u001b[0m val_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m==\u001b[39m batch_labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    108\u001b[0m val_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n",
    "    demo_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
