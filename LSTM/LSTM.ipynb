{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_len: int = 100):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Convert input to string and clean it\n",
    "        text = str(text)\n",
    "        text = re.sub(r'([<>/=\"])', r' \\1 ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            # Ensure text is string\n",
    "            text = str(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq and word not in self.vocab:\n",
    "                self.vocab[word] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def encode_text(self, text: str) -> List[int]:\n",
    "        # Ensure text is string\n",
    "        text = str(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend(['<PAD>'] * (self.max_len - len(tokens)))\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], preprocessor: TextPreprocessor):\n",
    "        # Convert all texts to strings\n",
    "        self.texts = [str(text) for text in texts]\n",
    "        self.preprocessor = preprocessor\n",
    "        self.encodings = [self.preprocessor.encode_text(text) for text in self.texts]\n",
    "        self.labels = [int(label) for label in labels]  # Convert labels to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "\n",
    "def load_and_clean_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load and clean the dataset, ensuring proper data types.\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert texts to strings and clean them\n",
    "        texts = [str(text).strip() for text in data['Sentence']]\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        labels = [int(label) for label in data['Label']]\n",
    "        \n",
    "        # Basic validation\n",
    "        assert len(texts) == len(labels), \"Number of texts and labels must match\"\n",
    "        assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n",
    "        assert all(isinstance(label, int) and label in [0, 1] for label in labels), \"Labels must be binary (0 or 1)\"\n",
    "        \n",
    "        print(f\"Loaded {len(texts)} samples successfully\")\n",
    "        \n",
    "        # Print some basic statistics\n",
    "        print(f\"Number of positive samples: {sum(labels)}\")\n",
    "        print(f\"Number of negative samples: {len(labels) - sum(labels)}\")\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetectorLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 50, \n",
    "                 hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSSDetector:\n",
    "    def __init__(self, max_len: int = 100, device: str = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else torch.device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.preprocessor = TextPreprocessor(max_len)\n",
    "        self.model = None\n",
    "        self.results = {}\n",
    "\n",
    "    def plot_loss_curves(self):\n",
    "        \"\"\"Plot loss curve visualizations comparing different learning rates.\"\"\"\n",
    "        # Individual plots for each learning rate\n",
    "        for lr in self.results.keys():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            train_losses = self.results[lr]['train_losses']\n",
    "            val_losses = self.results[lr]['val_losses']\n",
    "            epochs = range(1, len(train_losses) + 1)\n",
    "            \n",
    "            plt.plot(epochs, train_losses, label='Training Loss', marker='o', markersize=4)\n",
    "            plt.plot(epochs, val_losses, label='Validation Loss', marker='o', markersize=4)\n",
    "            \n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Training and Validation Loss (Learning Rate: {lr})')\n",
    "            plt.xticks(np.arange(0, len(train_losses) + 1, 5))\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'LSTM_loss_plot_lr_{lr}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Combined training losses plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lr in self.results.keys():\n",
    "            train_losses = self.results[lr]['train_losses']\n",
    "            epochs = range(1, len(train_losses) + 1)\n",
    "            plt.plot(epochs, train_losses, label=f'LR = {lr}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Training Loss Comparison Across Learning Rates')\n",
    "        plt.xticks(np.arange(0, max(len(self.results[lr]['train_losses']) for lr in self.results.keys()) + 1, 5))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('LSTM_combined_training_losses.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Combined validation losses plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lr in self.results.keys():\n",
    "            val_losses = self.results[lr]['val_losses']\n",
    "            epochs = range(1, len(val_losses) + 1)\n",
    "            plt.plot(epochs, val_losses, label=f'LR = {lr}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation Loss')\n",
    "        plt.title('Validation Loss Comparison Across Learning Rates')\n",
    "        plt.xticks(np.arange(0, max(len(self.results[lr]['val_losses']) for lr in self.results.keys()) + 1, 5))\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('LSTM_combined_validation_losses.png')\n",
    "        plt.close()\n",
    "\n",
    "    def train(self, texts: List[str], labels: List[int], \n",
    "              epochs: int = 50, batch_size: int = 16,  \n",
    "              learning_rates: List[float] = [0.001, 0.002, 0.01, 0.02, 0.05]):\n",
    "        try:\n",
    "            texts = [str(text) for text in texts]\n",
    "            labels = torch.tensor(labels, dtype=torch.float)\n",
    "            \n",
    "            self.preprocessor.build_vocab(texts)\n",
    "            dataset = XSSDataset(texts, labels.numpy(), self.preprocessor)\n",
    "            \n",
    "            # Split dataset\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            val_size = int(0.2 * len(dataset))\n",
    "            test_size = len(dataset) - train_size - val_size\n",
    "            \n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataset splits:\")\n",
    "            print(f\"Training: {train_size} samples\")\n",
    "            print(f\"Validation: {val_size} samples\")\n",
    "            print(f\"Test: {test_size} samples\")\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            # Import metrics calculation functions\n",
    "            from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "            \n",
    "            # Train with different learning rates\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\n--- Learning Rate: {lr} ---\")\n",
    "                \n",
    "                self.model = XSSDetectorLSTM(\n",
    "                    vocab_size=self.preprocessor.vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    dropout=0.3\n",
    "                ).to(self.device)\n",
    "                \n",
    "                optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "                criterion = nn.BCELoss()\n",
    "                \n",
    "                # Learning rate scheduler for better convergence\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "                )\n",
    "                \n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training\n",
    "                    self.model.train()\n",
    "                    total_loss = 0\n",
    "                    train_correct = 0\n",
    "                    train_total = 0\n",
    "                    \n",
    "                    for batch_sequences, batch_labels in train_loader:\n",
    "                        batch_sequences = batch_sequences.to(self.device)\n",
    "                        batch_labels = batch_labels.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = self.model(batch_sequences).squeeze()\n",
    "                        loss = criterion(outputs, batch_labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        predictions = (outputs >= 0.5).float()\n",
    "                        train_correct += (predictions == batch_labels).sum().item()\n",
    "                        train_total += len(batch_labels)\n",
    "                    \n",
    "                    avg_train_loss = total_loss / len(train_loader)\n",
    "                    train_losses.append(avg_train_loss)\n",
    "                    \n",
    "                    # Validation\n",
    "                    self.model.eval()\n",
    "                    val_loss = 0\n",
    "                    val_predictions = []\n",
    "                    val_true_labels = []\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for batch_sequences, batch_labels in val_loader:\n",
    "                            batch_sequences = batch_sequences.to(self.device)\n",
    "                            batch_labels = batch_labels.to(self.device)\n",
    "                            \n",
    "                            outputs = self.model(batch_sequences).squeeze()\n",
    "                            batch_val_loss = criterion(outputs, batch_labels).item()\n",
    "                            val_loss += batch_val_loss\n",
    "                            \n",
    "                            predictions = (outputs >= 0.5).float()\n",
    "                            val_predictions.extend(predictions.cpu().numpy())\n",
    "                            val_true_labels.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    \n",
    "                    print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "                    \n",
    "                    # Learning rate scheduler step\n",
    "                    scheduler.step(avg_val_loss)\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        patience_counter = 0\n",
    "                        # Save best model\n",
    "                        torch.save({\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'preprocessor': self.preprocessor\n",
    "                        }, f'LSTM_model_lr_{lr}.pth')\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    if patience_counter > 50:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                # Calculate final metrics\n",
    "                val_predictions = np.array(val_predictions)\n",
    "                val_true_labels = np.array(val_true_labels)\n",
    "                \n",
    "                f1 = f1_score(val_true_labels, val_predictions)\n",
    "                accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "                precision = precision_score(val_true_labels, val_predictions)\n",
    "                recall = recall_score(val_true_labels, val_predictions)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[lr] = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'f1_score': f1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall\n",
    "                }\n",
    "            \n",
    "            # Print comprehensive results\n",
    "            print(\"\\n--- Comprehensive Results ---\")\n",
    "            for lr in learning_rates:\n",
    "                r = self.results[lr]\n",
    "                print(f\"\\nLearning Rate: {lr}\")\n",
    "                print(f\"F1 Score: {r['f1_score']:.16f}\")\n",
    "                print(f\"Accuracy: {r['accuracy']:.16f}\")\n",
    "                print(f\"Precision: {r['precision']:.16f}\")\n",
    "                print(f\"Recall: {r['recall']:.16f}\")\n",
    "            \n",
    "            # Plot all loss curves\n",
    "            self.plot_loss_curves()\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Training error: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-execution environment check\n",
    "def check_environment():\n",
    "    print(\"\\n--- Environment Check ---\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    try:\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    except:\n",
    "        print(\"No CUDA device currently selected\")\n",
    "\n",
    "def demo_detector(dataset_path='../Training Dataset/final_dataset.csv'):\n",
    "    print(\"\\n--- XSS Detection Model Demonstration ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load and clean data\n",
    "        texts, labels = load_and_clean_data(dataset_path)\n",
    "        \n",
    "        # Initialize and train detector\n",
    "        detector = XSSDetector(max_len=100)\n",
    "        detector.train(\n",
    "            texts=texts,\n",
    "            labels=labels,\n",
    "            epochs=50,\n",
    "            batch_size=16,\n",
    "            learning_rates=[0.001, 0.002, 0.01, 0.02, 0.05]\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Demonstration failed: {e}\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. Ensure correct dataset path\")\n",
    "        print(\"2. Check dataset format\")\n",
    "        print(\"3. Verify required libraries are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Environment Check ---\n",
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "\n",
      "--- XSS Detection Model Demonstration ---\n",
      "Loaded 88310 samples successfully\n",
      "Number of positive samples: 50590\n",
      "Number of negative samples: 37720\n",
      "Using device: cuda\n",
      "\n",
      "Dataset splits:\n",
      "Training: 61816 samples\n",
      "Validation: 17662 samples\n",
      "Test: 8832 samples\n",
      "\n",
      "--- Learning Rate: 0.001 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0811, Val Loss = 0.0529\n",
      "Epoch 2/50: Train Loss = 0.0436, Val Loss = 0.0460\n",
      "Epoch 3/50: Train Loss = 0.0332, Val Loss = 0.0404\n",
      "Epoch 4/50: Train Loss = 0.0290, Val Loss = 0.0418\n",
      "Epoch 5/50: Train Loss = 0.0268, Val Loss = 0.0411\n",
      "Epoch 6/50: Train Loss = 0.0275, Val Loss = 0.0397\n",
      "Epoch 7/50: Train Loss = 0.0269, Val Loss = 0.0406\n",
      "Epoch 8/50: Train Loss = 0.0255, Val Loss = 0.0386\n",
      "Epoch 9/50: Train Loss = 0.0240, Val Loss = 0.0446\n",
      "Epoch 10/50: Train Loss = 0.0250, Val Loss = 0.0451\n",
      "Epoch 11/50: Train Loss = 0.0264, Val Loss = 0.0654\n",
      "Epoch 12/50: Train Loss = 0.0240, Val Loss = 0.0493\n",
      "Epoch 13/50: Train Loss = 0.0202, Val Loss = 0.0451\n",
      "Epoch 14/50: Train Loss = 0.0206, Val Loss = 0.0405\n",
      "Epoch 15/50: Train Loss = 0.0216, Val Loss = 0.0424\n",
      "Epoch 16/50: Train Loss = 0.0214, Val Loss = 0.0433\n",
      "Epoch 17/50: Train Loss = 0.0188, Val Loss = 0.0407\n",
      "Epoch 18/50: Train Loss = 0.0187, Val Loss = 0.0423\n",
      "Epoch 19/50: Train Loss = 0.0185, Val Loss = 0.0454\n",
      "Epoch 20/50: Train Loss = 0.0191, Val Loss = 0.0440\n",
      "Epoch 21/50: Train Loss = 0.0175, Val Loss = 0.0477\n",
      "Epoch 22/50: Train Loss = 0.0170, Val Loss = 0.0475\n",
      "Epoch 23/50: Train Loss = 0.0173, Val Loss = 0.0491\n",
      "Epoch 24/50: Train Loss = 0.0170, Val Loss = 0.0548\n",
      "Epoch 25/50: Train Loss = 0.0169, Val Loss = 0.0496\n",
      "Epoch 26/50: Train Loss = 0.0163, Val Loss = 0.0515\n",
      "Epoch 27/50: Train Loss = 0.0165, Val Loss = 0.0509\n",
      "Epoch 28/50: Train Loss = 0.0168, Val Loss = 0.0513\n",
      "Epoch 29/50: Train Loss = 0.0165, Val Loss = 0.0511\n",
      "Epoch 30/50: Train Loss = 0.0159, Val Loss = 0.0534\n",
      "Epoch 31/50: Train Loss = 0.0161, Val Loss = 0.0542\n",
      "Epoch 32/50: Train Loss = 0.0158, Val Loss = 0.0587\n",
      "Epoch 33/50: Train Loss = 0.0157, Val Loss = 0.0593\n",
      "Epoch 34/50: Train Loss = 0.0157, Val Loss = 0.0594\n",
      "Epoch 35/50: Train Loss = 0.0157, Val Loss = 0.0608\n",
      "Epoch 36/50: Train Loss = 0.0157, Val Loss = 0.0611\n",
      "Epoch 37/50: Train Loss = 0.0158, Val Loss = 0.0622\n",
      "Epoch 38/50: Train Loss = 0.0157, Val Loss = 0.0617\n",
      "Epoch 39/50: Train Loss = 0.0159, Val Loss = 0.0613\n",
      "Epoch 40/50: Train Loss = 0.0155, Val Loss = 0.0619\n",
      "Epoch 41/50: Train Loss = 0.0157, Val Loss = 0.0617\n",
      "Epoch 42/50: Train Loss = 0.0155, Val Loss = 0.0622\n",
      "Epoch 43/50: Train Loss = 0.0156, Val Loss = 0.0622\n",
      "Epoch 44/50: Train Loss = 0.0154, Val Loss = 0.0623\n",
      "Epoch 45/50: Train Loss = 0.0155, Val Loss = 0.0621\n",
      "Epoch 46/50: Train Loss = 0.0158, Val Loss = 0.0621\n",
      "Epoch 47/50: Train Loss = 0.0155, Val Loss = 0.0621\n",
      "Epoch 48/50: Train Loss = 0.0154, Val Loss = 0.0621\n",
      "Epoch 49/50: Train Loss = 0.0155, Val Loss = 0.0623\n",
      "Epoch 50/50: Train Loss = 0.0154, Val Loss = 0.0623\n",
      "\n",
      "--- Learning Rate: 0.002 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0709, Val Loss = 0.0478\n",
      "Epoch 2/50: Train Loss = 0.0390, Val Loss = 0.0406\n",
      "Epoch 3/50: Train Loss = 0.0325, Val Loss = 0.0393\n",
      "Epoch 4/50: Train Loss = 0.0307, Val Loss = 0.0426\n",
      "Epoch 5/50: Train Loss = 0.0288, Val Loss = 0.0470\n",
      "Epoch 6/50: Train Loss = 0.0387, Val Loss = 0.0446\n",
      "Epoch 7/50: Train Loss = 0.0282, Val Loss = 0.0383\n",
      "Epoch 8/50: Train Loss = 0.0269, Val Loss = 0.0418\n",
      "Epoch 9/50: Train Loss = 0.0279, Val Loss = 0.0396\n",
      "Epoch 10/50: Train Loss = 0.0255, Val Loss = 0.0451\n",
      "Epoch 11/50: Train Loss = 0.0270, Val Loss = 0.0399\n",
      "Epoch 12/50: Train Loss = 0.0228, Val Loss = 0.0452\n",
      "Epoch 13/50: Train Loss = 0.0227, Val Loss = 0.0438\n",
      "Epoch 14/50: Train Loss = 0.0230, Val Loss = 0.0413\n",
      "Epoch 15/50: Train Loss = 0.0230, Val Loss = 0.0410\n",
      "Epoch 16/50: Train Loss = 0.0198, Val Loss = 0.0432\n",
      "Epoch 17/50: Train Loss = 0.0200, Val Loss = 0.0431\n",
      "Epoch 18/50: Train Loss = 0.0205, Val Loss = 0.0440\n",
      "Epoch 19/50: Train Loss = 0.0206, Val Loss = 0.0434\n",
      "Epoch 20/50: Train Loss = 0.0190, Val Loss = 0.0431\n",
      "Epoch 21/50: Train Loss = 0.0187, Val Loss = 0.0449\n",
      "Epoch 22/50: Train Loss = 0.0187, Val Loss = 0.0483\n",
      "Epoch 23/50: Train Loss = 0.0184, Val Loss = 0.0467\n",
      "Epoch 24/50: Train Loss = 0.0181, Val Loss = 0.0483\n",
      "Epoch 25/50: Train Loss = 0.0180, Val Loss = 0.0488\n",
      "Epoch 26/50: Train Loss = 0.0178, Val Loss = 0.0505\n",
      "Epoch 27/50: Train Loss = 0.0180, Val Loss = 0.0511\n",
      "Epoch 28/50: Train Loss = 0.0171, Val Loss = 0.0508\n",
      "Epoch 29/50: Train Loss = 0.0171, Val Loss = 0.0531\n",
      "Epoch 30/50: Train Loss = 0.0171, Val Loss = 0.0529\n",
      "Epoch 31/50: Train Loss = 0.0169, Val Loss = 0.0546\n",
      "Epoch 32/50: Train Loss = 0.0167, Val Loss = 0.0537\n",
      "Epoch 33/50: Train Loss = 0.0169, Val Loss = 0.0548\n",
      "Epoch 34/50: Train Loss = 0.0168, Val Loss = 0.0542\n",
      "Epoch 35/50: Train Loss = 0.0167, Val Loss = 0.0548\n",
      "Epoch 36/50: Train Loss = 0.0169, Val Loss = 0.0554\n",
      "Epoch 37/50: Train Loss = 0.0170, Val Loss = 0.0557\n",
      "Epoch 38/50: Train Loss = 0.0164, Val Loss = 0.0565\n",
      "Epoch 39/50: Train Loss = 0.0164, Val Loss = 0.0572\n",
      "Epoch 40/50: Train Loss = 0.0165, Val Loss = 0.0570\n",
      "Epoch 41/50: Train Loss = 0.0165, Val Loss = 0.0572\n",
      "Epoch 42/50: Train Loss = 0.0165, Val Loss = 0.0574\n",
      "Epoch 43/50: Train Loss = 0.0166, Val Loss = 0.0572\n",
      "Epoch 44/50: Train Loss = 0.0165, Val Loss = 0.0576\n",
      "Epoch 45/50: Train Loss = 0.0165, Val Loss = 0.0577\n",
      "Epoch 46/50: Train Loss = 0.0165, Val Loss = 0.0578\n",
      "Epoch 47/50: Train Loss = 0.0163, Val Loss = 0.0576\n",
      "Epoch 48/50: Train Loss = 0.0163, Val Loss = 0.0578\n",
      "Epoch 49/50: Train Loss = 0.0163, Val Loss = 0.0578\n",
      "Epoch 50/50: Train Loss = 0.0167, Val Loss = 0.0578\n",
      "\n",
      "--- Learning Rate: 0.01 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.0884, Val Loss = 0.0574\n",
      "Epoch 2/50: Train Loss = 0.0554, Val Loss = 0.0539\n",
      "Epoch 3/50: Train Loss = 0.0530, Val Loss = 0.0465\n",
      "Epoch 4/50: Train Loss = 0.0508, Val Loss = 0.1356\n",
      "Epoch 5/50: Train Loss = 0.0543, Val Loss = 0.0513\n",
      "Epoch 6/50: Train Loss = 0.0511, Val Loss = 0.0482\n",
      "Epoch 7/50: Train Loss = 0.0495, Val Loss = 0.0468\n",
      "Epoch 8/50: Train Loss = 0.0367, Val Loss = 0.0426\n",
      "Epoch 9/50: Train Loss = 0.0347, Val Loss = 0.0434\n",
      "Epoch 10/50: Train Loss = 0.0358, Val Loss = 0.0427\n",
      "Epoch 11/50: Train Loss = 0.0345, Val Loss = 0.0456\n",
      "Epoch 12/50: Train Loss = 0.0354, Val Loss = 0.0474\n",
      "Epoch 13/50: Train Loss = 0.0285, Val Loss = 0.0440\n",
      "Epoch 14/50: Train Loss = 0.0286, Val Loss = 0.0457\n",
      "Epoch 15/50: Train Loss = 0.0276, Val Loss = 0.0499\n",
      "Epoch 16/50: Train Loss = 0.0275, Val Loss = 0.0408\n",
      "Epoch 17/50: Train Loss = 0.0270, Val Loss = 0.0401\n",
      "Epoch 18/50: Train Loss = 0.0271, Val Loss = 0.0428\n",
      "Epoch 19/50: Train Loss = 0.0267, Val Loss = 0.0414\n",
      "Epoch 20/50: Train Loss = 0.0265, Val Loss = 0.0459\n",
      "Epoch 21/50: Train Loss = 0.0277, Val Loss = 0.0450\n",
      "Epoch 22/50: Train Loss = 0.0228, Val Loss = 0.0440\n",
      "Epoch 23/50: Train Loss = 0.0224, Val Loss = 0.0489\n",
      "Epoch 24/50: Train Loss = 0.0230, Val Loss = 0.0449\n",
      "Epoch 25/50: Train Loss = 0.0229, Val Loss = 0.0457\n",
      "Epoch 26/50: Train Loss = 0.0205, Val Loss = 0.0475\n",
      "Epoch 27/50: Train Loss = 0.0206, Val Loss = 0.0452\n",
      "Epoch 28/50: Train Loss = 0.0207, Val Loss = 0.0440\n",
      "Epoch 29/50: Train Loss = 0.0208, Val Loss = 0.0453\n",
      "Epoch 30/50: Train Loss = 0.0191, Val Loss = 0.0496\n",
      "Epoch 31/50: Train Loss = 0.0191, Val Loss = 0.0472\n",
      "Epoch 32/50: Train Loss = 0.0187, Val Loss = 0.0664\n",
      "Epoch 33/50: Train Loss = 0.0188, Val Loss = 0.0510\n",
      "Epoch 34/50: Train Loss = 0.0182, Val Loss = 0.0537\n",
      "Epoch 35/50: Train Loss = 0.0182, Val Loss = 0.0561\n",
      "Epoch 36/50: Train Loss = 0.0180, Val Loss = 0.0568\n",
      "Epoch 37/50: Train Loss = 0.0176, Val Loss = 0.0564\n",
      "Epoch 38/50: Train Loss = 0.0173, Val Loss = 0.0582\n",
      "Epoch 39/50: Train Loss = 0.0174, Val Loss = 0.0624\n",
      "Epoch 40/50: Train Loss = 0.0172, Val Loss = 0.0615\n",
      "Epoch 41/50: Train Loss = 0.0173, Val Loss = 0.0593\n",
      "Epoch 42/50: Train Loss = 0.0171, Val Loss = 0.0609\n",
      "Epoch 43/50: Train Loss = 0.0171, Val Loss = 0.0606\n",
      "Epoch 44/50: Train Loss = 0.0172, Val Loss = 0.0633\n",
      "Epoch 45/50: Train Loss = 0.0168, Val Loss = 0.0634\n",
      "Epoch 46/50: Train Loss = 0.0168, Val Loss = 0.0653\n",
      "Epoch 47/50: Train Loss = 0.0165, Val Loss = 0.0668\n",
      "Epoch 48/50: Train Loss = 0.0165, Val Loss = 0.0674\n",
      "Epoch 49/50: Train Loss = 0.0167, Val Loss = 0.0678\n",
      "Epoch 50/50: Train Loss = 0.0167, Val Loss = 0.0682\n",
      "\n",
      "--- Learning Rate: 0.02 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.1097, Val Loss = 0.0751\n",
      "Epoch 2/50: Train Loss = 0.0923, Val Loss = 0.1212\n",
      "Epoch 3/50: Train Loss = 0.0796, Val Loss = 0.0849\n",
      "Epoch 4/50: Train Loss = 0.0864, Val Loss = 0.0671\n",
      "Epoch 5/50: Train Loss = 0.0809, Val Loss = 0.0830\n",
      "Epoch 6/50: Train Loss = 0.0845, Val Loss = 0.0900\n",
      "Epoch 7/50: Train Loss = 0.0896, Val Loss = 0.0907\n",
      "Epoch 8/50: Train Loss = 0.0769, Val Loss = 0.0740\n",
      "Epoch 9/50: Train Loss = 0.0613, Val Loss = 0.0560\n",
      "Epoch 10/50: Train Loss = 0.0574, Val Loss = 0.0516\n",
      "Epoch 11/50: Train Loss = 0.0545, Val Loss = 0.0485\n",
      "Epoch 12/50: Train Loss = 0.0536, Val Loss = 0.0525\n",
      "Epoch 13/50: Train Loss = 0.0521, Val Loss = 0.0514\n",
      "Epoch 14/50: Train Loss = 0.0530, Val Loss = 0.0564\n",
      "Epoch 15/50: Train Loss = 0.0533, Val Loss = 0.0547\n",
      "Epoch 16/50: Train Loss = 0.0419, Val Loss = 0.0478\n",
      "Epoch 17/50: Train Loss = 0.0409, Val Loss = 0.0471\n",
      "Epoch 18/50: Train Loss = 0.0395, Val Loss = 0.0442\n",
      "Epoch 19/50: Train Loss = 0.0381, Val Loss = 0.0441\n",
      "Epoch 20/50: Train Loss = 0.0366, Val Loss = 0.0420\n",
      "Epoch 21/50: Train Loss = 0.0397, Val Loss = 0.0469\n",
      "Epoch 22/50: Train Loss = 0.0355, Val Loss = 0.0481\n",
      "Epoch 23/50: Train Loss = 0.0350, Val Loss = 0.0425\n",
      "Epoch 24/50: Train Loss = 0.0359, Val Loss = 0.0403\n",
      "Epoch 25/50: Train Loss = 0.0332, Val Loss = 0.0398\n",
      "Epoch 26/50: Train Loss = 0.0316, Val Loss = 0.0399\n",
      "Epoch 27/50: Train Loss = 0.0331, Val Loss = 0.0471\n",
      "Epoch 28/50: Train Loss = 0.0312, Val Loss = 0.0436\n",
      "Epoch 29/50: Train Loss = 0.0382, Val Loss = 0.0578\n",
      "Epoch 30/50: Train Loss = 0.0304, Val Loss = 0.0418\n",
      "Epoch 31/50: Train Loss = 0.0279, Val Loss = 0.0415\n",
      "Epoch 32/50: Train Loss = 0.0289, Val Loss = 0.0374\n",
      "Epoch 33/50: Train Loss = 0.0265, Val Loss = 0.0399\n",
      "Epoch 34/50: Train Loss = 0.0262, Val Loss = 0.0425\n",
      "Epoch 35/50: Train Loss = 0.0263, Val Loss = 0.0412\n",
      "Epoch 36/50: Train Loss = 0.0264, Val Loss = 0.0405\n",
      "Epoch 37/50: Train Loss = 0.0231, Val Loss = 0.0384\n",
      "Epoch 38/50: Train Loss = 0.0223, Val Loss = 0.0421\n",
      "Epoch 39/50: Train Loss = 0.0231, Val Loss = 0.0399\n",
      "Epoch 40/50: Train Loss = 0.0226, Val Loss = 0.0455\n",
      "Epoch 41/50: Train Loss = 0.0208, Val Loss = 0.0431\n",
      "Epoch 42/50: Train Loss = 0.0203, Val Loss = 0.0427\n",
      "Epoch 43/50: Train Loss = 0.0210, Val Loss = 0.0430\n",
      "Epoch 44/50: Train Loss = 0.0208, Val Loss = 0.0398\n",
      "Epoch 45/50: Train Loss = 0.0189, Val Loss = 0.0446\n",
      "Epoch 46/50: Train Loss = 0.0192, Val Loss = 0.0442\n",
      "Epoch 47/50: Train Loss = 0.0192, Val Loss = 0.0453\n",
      "Epoch 48/50: Train Loss = 0.0188, Val Loss = 0.0477\n",
      "Epoch 49/50: Train Loss = 0.0183, Val Loss = 0.0462\n",
      "Epoch 50/50: Train Loss = 0.0182, Val Loss = 0.0484\n",
      "\n",
      "--- Learning Rate: 0.05 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is1ab/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 0.2198, Val Loss = 0.1610\n",
      "Epoch 2/50: Train Loss = 0.2011, Val Loss = 0.1378\n",
      "Epoch 3/50: Train Loss = 0.1815, Val Loss = 0.1214\n",
      "Epoch 4/50: Train Loss = 0.1822, Val Loss = 0.1144\n",
      "Epoch 5/50: Train Loss = 0.1955, Val Loss = 0.1628\n",
      "Epoch 6/50: Train Loss = 0.1852, Val Loss = 0.1376\n",
      "Epoch 7/50: Train Loss = 0.1918, Val Loss = 0.1584\n",
      "Epoch 8/50: Train Loss = 0.1995, Val Loss = 0.1385\n",
      "Epoch 9/50: Train Loss = 0.1507, Val Loss = 0.1100\n",
      "Epoch 10/50: Train Loss = 0.1475, Val Loss = 0.1118\n",
      "Epoch 11/50: Train Loss = 0.1336, Val Loss = 0.1048\n",
      "Epoch 12/50: Train Loss = 0.1282, Val Loss = 0.1115\n",
      "Epoch 13/50: Train Loss = 0.1211, Val Loss = 0.0958\n",
      "Epoch 14/50: Train Loss = 0.1244, Val Loss = 0.1066\n",
      "Epoch 15/50: Train Loss = 0.1398, Val Loss = 0.1291\n",
      "Epoch 16/50: Train Loss = 0.1705, Val Loss = 0.1148\n",
      "Epoch 17/50: Train Loss = 0.1260, Val Loss = 0.1007\n",
      "Epoch 18/50: Train Loss = 0.1067, Val Loss = 0.0850\n",
      "Epoch 19/50: Train Loss = 0.0946, Val Loss = 0.0769\n",
      "Epoch 20/50: Train Loss = 0.0929, Val Loss = 0.0747\n",
      "Epoch 21/50: Train Loss = 0.0867, Val Loss = 0.0799\n",
      "Epoch 22/50: Train Loss = 0.0842, Val Loss = 0.0731\n",
      "Epoch 23/50: Train Loss = 0.0834, Val Loss = 0.0722\n",
      "Epoch 24/50: Train Loss = 0.0826, Val Loss = 0.0748\n",
      "Epoch 25/50: Train Loss = 0.0826, Val Loss = 0.0762\n",
      "Epoch 26/50: Train Loss = 0.0824, Val Loss = 0.0707\n",
      "Epoch 27/50: Train Loss = 0.0776, Val Loss = 0.0748\n",
      "Epoch 28/50: Train Loss = 0.0807, Val Loss = 0.0730\n",
      "Epoch 29/50: Train Loss = 0.0744, Val Loss = 0.0670\n",
      "Epoch 30/50: Train Loss = 0.0762, Val Loss = 0.0775\n",
      "Epoch 31/50: Train Loss = 0.0814, Val Loss = 0.0815\n",
      "Epoch 32/50: Train Loss = 0.0787, Val Loss = 0.0723\n",
      "Epoch 33/50: Train Loss = 0.0760, Val Loss = 0.0701\n",
      "Epoch 34/50: Train Loss = 0.0671, Val Loss = 0.0623\n",
      "Epoch 35/50: Train Loss = 0.0615, Val Loss = 0.0585\n",
      "Epoch 36/50: Train Loss = 0.0583, Val Loss = 0.0615\n",
      "Epoch 37/50: Train Loss = 0.0561, Val Loss = 0.0584\n",
      "Epoch 38/50: Train Loss = 0.0555, Val Loss = 0.0556\n",
      "Epoch 39/50: Train Loss = 0.0544, Val Loss = 0.0568\n",
      "Epoch 40/50: Train Loss = 0.0541, Val Loss = 0.0551\n",
      "Epoch 41/50: Train Loss = 0.0519, Val Loss = 0.0509\n",
      "Epoch 42/50: Train Loss = 0.0516, Val Loss = 0.0532\n",
      "Epoch 43/50: Train Loss = 0.0480, Val Loss = 0.0494\n",
      "Epoch 44/50: Train Loss = 0.0467, Val Loss = 0.0568\n",
      "Epoch 45/50: Train Loss = 0.0472, Val Loss = 0.0508\n",
      "Epoch 46/50: Train Loss = 0.0470, Val Loss = 0.0460\n",
      "Epoch 47/50: Train Loss = 0.0476, Val Loss = 0.0561\n",
      "Epoch 48/50: Train Loss = 0.0486, Val Loss = 0.0554\n",
      "Epoch 49/50: Train Loss = 0.0493, Val Loss = 0.0532\n",
      "Epoch 50/50: Train Loss = 0.0500, Val Loss = 0.0561\n",
      "\n",
      "--- Comprehensive Results ---\n",
      "\n",
      "Learning Rate: 0.001\n",
      "F1 Score: 0.9877951974598135\n",
      "Accuracy: 0.9860717925489751\n",
      "Precision: 0.9925224327018943\n",
      "Recall: 0.9831127789847917\n",
      "\n",
      "Learning Rate: 0.002\n",
      "F1 Score: 0.9866693096783785\n",
      "Accuracy: 0.9847695617710338\n",
      "Precision: 0.9902516661693027\n",
      "Recall: 0.9831127789847917\n",
      "\n",
      "Learning Rate: 0.01\n",
      "F1 Score: 0.9871743885464307\n",
      "Accuracy: 0.9853923677952666\n",
      "Precision: 0.9938938938938939\n",
      "Recall: 0.9805451313450524\n",
      "\n",
      "Learning Rate: 0.02\n",
      "F1 Score: 0.9875514556365620\n",
      "Accuracy: 0.9857886989015966\n",
      "Precision: 0.9919298595197769\n",
      "Recall: 0.9832115346632432\n",
      "\n",
      "Learning Rate: 0.05\n",
      "F1 Score: 0.9849987540493397\n",
      "Accuracy: 0.9829577624278111\n",
      "Precision: 0.9942650166012678\n",
      "Recall: 0.9759036144578314\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    check_environment()\n",
    "    demo_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
